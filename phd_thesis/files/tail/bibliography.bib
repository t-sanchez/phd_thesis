
@article{pipe1999sampling,
  title={Sampling density compensation in MRI: rationale and an iterative numerical solution},
  author={Pipe, James G and Menon, Padmanabhan},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={41},
  number={1},
  pages={179--186},
  year={1999},
  publisher={Wiley Online Library}
}

@article{vannesjo2013gradient,
  title={Gradient system characterization by impulse response measurements with a dynamic field camera},
  author={Vannesjo, Signe J and Haeberlin, Maximilan and Kasper, Lars and Pavan, Matteo and Wilm, Bertram J and Barmet, Christoph and Pruessmann, Klaas P},
  journal={Magnetic resonance in medicine},
  volume={69},
  number={2},
  pages={583--593},
  year={2013},
  publisher={Wiley Online Library}
}


@inproceedings{chaithya2022hybrid,
  title={Hybrid learning of Non-Cartesian k-space trajectory and MR image reconstruction networks},
  author={Chaithya, GR and Ramzi, Zaccharie and Ciuciu, Philippe},
  booktitle={2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)},
  pages={1--5},
  year={2022},
  organization={IEEE}
}


@article{ramzi2022nc,
  title={NC-PDNet: a Density-Compensated Unrolled Network for 2D and 3D non-Cartesian MRI Reconstruction},
  author={Ramzi, Zaccharie and Chaithya, GR and Starck, Jean-Luc and Ciuciu, Philippe},
  journal={IEEE Transactions on Medical Imaging},
  year={2022},
  publisher={IEEE}
}

@inproceedings{hassidim2017submodular,
  title={Submodular optimization under noise},
  author={Hassidim, Avinatan and Singer, Yaron},
  booktitle={Conference on Learning Theory},
  pages={1069--1122},
  year={2017},
  organization={PMLR}
}


@article{coppo2015free,
  title={Free-running 4D whole-heart self-navigated golden angle MRI: initial results},
  author={Coppo, Simone and Piccini, Davide and Bonanno, Gabriele and Chaptinel, J{\'e}r{\^o}me and Vincenti, Gabriella and Feliciano, H{\'e}l{\`e}ne and Van Heeswijk, Ruud B and Schwitter, Juerg and Stuber, Matthias},
  journal={Magnetic resonance in medicine},
  volume={74},
  number={5},
  pages={1306--1316},
  year={2015},
  publisher={Wiley Online Library}
}


@inproceedings{singla2016noisy,
  title={Noisy submodular maximization via adaptive sampling with applications to crowdsourced image collection summarization},
  author={Singla, Adish and Tschiatschek, Sebastian and Krause, Andreas},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@article{edelstein2010mri,
  title={MRI: time is dose—and money and versatility},
  author={Edelstein, William A and Mahesh, Mahadevappa and Carrino, John A},
  journal={Journal of the American College of Radiology},
  volume={7},
  number={8},
  pages={650--652},
  year={2010},
  publisher={Elsevier}
}


@article{liang2009accelerating,
  title={Accelerating SENSE using compressed sensing},
  author={Liang, Dong and Liu, Bo and Wang, Jiunjie and Ying, Leslie},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={62},
  number={6},
  pages={1574--1584},
  year={2009},
  publisher={Wiley Online Library}
}


@inproceedings{zhu2017toward,
  title={Toward multimodal image-to-image translation},
  author={Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A and Wang, Oliver and Shechtman, Eli},
  booktitle={Advances in neural information processing systems},
  pages={465--476},
  year={2017}
}


@inproceedings{zhu2017unpaired,
  title={Unpaired I2I translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2223--2232},
  year={2017}
}
@inproceedings{choi2018stargan,
  title={Stargan: Unified GANs for multi-domain image-to-image translation},
  author={Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8789--8797},
  year={2018}
}

@inproceedings{sharma2020space,
  title={Space Filling Curves for MRI Sampling},
  author={Sharma, Shubham and Hari, KVS and Leus, Geert},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1115--1119},
  year={2020},
  organization={IEEE}
}

@inproceedings{sharma2020k,
  title={K-Space Trajectory Design for Reduced MRI Scan Time},
  author={Sharma, Shubham and Hari, KVS and Leus, Geert},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1120--1124},
  year={2020},
  organization={IEEE}
}

@article{hennig1986rare,
  title={RARE imaging: a fast imaging method for clinical MR},
  author={Hennig, J{\"u}rgen and Nauerth, A and Friedburg, HRARE},
  journal={Magnetic resonance in medicine},
  volume={3},
  number={6},
  pages={823--833},
  year={1986},
  publisher={Wiley Online Library}
}


@article{wright1997magnetic,
  author={Wright, G.A.},
  journal={IEEE Signal Processing Magazine}, 
  title={Magnetic resonance imaging}, 
  year={1997},
  volume={14},
  number={1},
  pages={56-66},
  doi={10.1109/79.560324}}

@inproceedings{sun2021plug,
  title={A plug-and-play deep image prior},
  author={Sun, Zhaodong and Latorre, Fabian and Sanchez, Thomas and Cevher, Volkan},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8103--8107},
  year={2021},
  organization={IEEE}
}

@article{nalisnick2018deep,
  title={Do deep generative models know what they don't know?},
  author={Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1810.09136},
  year={2018}
}


@article{chaithya2020optimizing,
  title={Optimizing full 3D SPARKLING trajectories for high-resolution T2*-weighted Magnetic Resonance Imaging},
  author={Chaithya, GR and Weiss, Pierre and Daval-Fr{\'e}rot, Guillaume and Massire, Aur{\'e}lien and Vignaud, Alexandre and Ciuciu, Philippe},
  year={2020}
}


@article{chaithya2022benchmarking,
  title={Benchmarking learned non-Cartesian k-space trajectories and reconstruction networks},
  author={Chaithya, GR and Ciuciu, Philippe},
  year={2022}
}


@article{bahadir2020deep,
  title={Deep-learning-based optimization of the under-sampling pattern in MRI},
  author={Bahadir, Cagla D and Wang, Alan Q and Dalca, Adrian V and Sabuncu, Mert R},
  journal={IEEE Transactions on Computational Imaging},
  volume={6},
  pages={1139--1152},
  year={2020},
  publisher={IEEE}
}


@inproceedings{chaithya2021learning,
  title={Learning the sampling density in 2D SPARKLING MRI acquisition for optimized image reconstruction},
  author={Chaithya, GR and Ramzi, Zaccharie and Ciuciu, Philippe},
  booktitle={2021 29th European Signal Processing Conference (EUSIPCO)},
  pages={960--964},
  year={2021},
  organization={IEEE}
}



@article{feng20185d,
  title={5D whole-heart sparse MRI},
  author={Feng, LI and Coppo, Simone and Piccini, Davide and Yerly, Jerome and Lim, Ruth P and Masci, Pier Giorgio and Stuber, Matthias and Sodickson, Daniel K and Otazo, Ricardo},
  journal={Magnetic resonance in medicine},
  volume={79},
  number={2},
  pages={826--838},
  year={2018},
  publisher={Wiley Online Library}
}


@article{sharma2021opt,
  title={OPT: Optimized Projection based k-space Trajectory Design for MR Image Reconstruction},
  author={Sharma, Shubham and Chatterjee, Sudhanya and Shanbhag, Dattesh D and Aggarwal, Hemant Kumar and Hari, KVS},
  year={2021}
}


@article{weiss2019pilot,
  title={PILOT: Physics-informed learned optimal trajectories for accelerated MRI},
  author={Weiss, Tomer and Senouf, Ortal and Vedula, Sanketh and Michailovich, Oleg and Zibulevsky, Michael and Bronstein, Alex},
  journal={arXiv preprint arXiv:1909.05773},
  year={2019}
}


@article{wang2021b,
  title={B-spline parameterized joint optimization of reconstruction and k-space trajectories (BJORK) for accelerated 2d MRI},
  author={Wang, Guanhua and Luo, Tianrui and Nielsen, Jon-Fredrik and Noll, Douglas C and Fessler, Jeffrey A},
  journal={arXiv preprint arXiv:2101.11369},
  year={2021}
}


@article{antun2020instabilities,
  title={On instabilities of deep learning in image reconstruction and the potential costs of AI},
  author={Antun, Vegard and Renna, Francesco and Poon, Clarice and Adcock, Ben and Hansen, Anders C},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30088--30095},
  year={2020},
  publisher={National Acad Sciences}
}


@inproceedings{johnson2021evaluation,
  title={Evaluation of the Robustness of Learned MR Image Reconstruction to Systematic Deviations Between Training and Test Data for the Models from the fastMRI Challenge},
  author={Johnson, Patricia M and Jeong, Geunu and Hammernik, Kerstin and Schlemper, Jo and Qin, Chen and Duan, Jinming and Rueckert, Daniel and Lee, Jingu and Pezzotti, Nicola and Weerdt, Elwin De and others},
  booktitle={International Workshop on Machine Learning for Medical Image Reconstruction},
  pages={25--34},
  year={2021},
  organization={Springer}
}


@article{recht2020using,
  title={Using deep learning to accelerate knee MRI at 3 T: results of an interchangeability study},
  author={Recht, Michael P and Zbontar, Jure and Sodickson, Daniel K and Knoll, Florian and Yakubova, Nafissa and Sriram, Anuroop and Murrell, Tullie and Defazio, Aaron and Rabbat, Michael and Rybak, Leon and others},
  journal={AJR. American journal of roentgenology},
  volume={215},
  number={6},
  pages={1421},
  year={2020},
  publisher={NIH Public Access}
}


@article{yu2022validation,
  title={Validation and Generalizability of Self-Supervised Image Reconstruction Methods for Undersampled MRI},
  author={Yu, Thomas and Hilbert, Tom and Piredda, Gian Franco and Joseph, Arun and Bonanno, Gabriele and Zenkhri, Salim and Omoumi, Patrick and Cuadra, Meritxell Bach and Canales-Rodr{\'\i}guez, Erick Jorge and Kober, Tobias and others},
  journal={arXiv preprint arXiv:2201.12535},
  year={2022}
}


@article{chen2022ai,
  title={AI-Based Reconstruction for Fast MRI—A Systematic Review and Meta-Analysis},
  author={Chen, Yutong and Sch{\"o}nlieb, Carola-Bibiane and Li{\`o}, Pietro and Leiner, Tim and Dragotti, Pier Luigi and Wang, Ge and Rueckert, Daniel and Firmin, David and Yang, Guang},
  journal={Proceedings of the IEEE},
  volume={110},
  number={2},
  pages={224--245},
  year={2022},
  publisher={IEEE}
}


@inproceedings{jung2007generalized,
  title={Generalized kt BLAST and kt SENSE using FOCUSS},
  author={Jung, Hong and Yoo, Jaeheung and Ye, Jong Chul},
  booktitle={2007 4th IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
  pages={145--148},
  year={2007},
  organization={IEEE}
}


@article{nemhauser1978best,
  title={Best algorithms for approximating the maximum of a submodular set function},
  author={Nemhauser, George L and Wolsey, Laurence A},
  journal={Mathematics of operations research},
  volume={3},
  number={3},
  pages={177--188},
  year={1978},
  publisher={INFORMS}
}
@article{echternach2010vocal,
  title={Vocal tract in female registers—a dynamic real-time MRI study},
  author={Echternach, Matthias and Sundberg, Johan and Arndt, Susan and Markl, Michael and Schumacher, Martin and Richter, Bernhard},
  journal={Journal of Voice},
  volume={24},
  number={2},
  pages={133--139},
  year={2010},
  publisher={Elsevier}
}


@article{majumdar2012calibration,
  title={Calibration-less multi-coil MR image reconstruction},
  author={Majumdar, Angshul and Ward, Rabab K},
  journal={Magnetic resonance imaging},
  volume={30},
  number={7},
  pages={1032--1045},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{trzasko2011calibrationless,
  title={Calibrationless parallel MRI using CLEAR},
  author={Trzasko, Joshua D and Manduca, Armando},
  booktitle={2011 conference record of the forty fifth Asilomar conference on signals, systems and computers (ASILOMAR)},
  pages={75--79},
  year={2011},
  organization={IEEE}
}


@inproceedings{el2019calibrationless,
  title={Calibrationless oscar-based image reconstruction in compressed sensing parallel MRI},
  author={El Gueddari, Loubna and Ciuciu, Philippe and Chouzenoux, Emilie and Vignaud, Alexandre and Pesquet, J-C},
  booktitle={2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)},
  pages={1532--1536},
  year={2019},
  organization={IEEE}
}


@article{meyer1992fast,
  title={Fast spiral coronary artery imaging},
  author={Meyer, Craig H and Hu, Bob S and Nishimura, Dwight G and Macovski, Albert},
  journal={Magnetic resonance in medicine},
  volume={28},
  number={2},
  pages={202--213},
  year={1992},
  publisher={Wiley Online Library}
}

@article{damadian1971tumor,
  title={Tumor detection by nuclear magnetic resonance},
  author={Damadian, Raymond},
  journal={Science},
  volume={171},
  number={3976},
  pages={1151--1153},
  year={1971},
  publisher={American Association for the Advancement of Science}
}


@article{rabi1938new,
  title={A new method of measuring nuclear magnetic moment},
  author={Rabi, Isidor Isaac and Zacharias, Jerrold R and Millman, Sidney and Kusch, Polykarp},
  journal={Physical review},
  volume={53},
  number={4},
  pages={318},
  year={1938},
  publisher={APS}
}


@article{bloch1946nuclear,
  title={Nuclear induction},
  author={Bloch, Felix},
  journal={Physical review},
  volume={70},
  number={7-8},
  pages={460},
  year={1946},
  publisher={APS}
}

@article{mansfield1973nmr,
  title={NMR'diffraction'in solids?},
  author={Mansfield, Peter and Grannell, Peter K},
  journal={Journal of Physics C: solid state physics},
  volume={6},
  number={22},
  pages={L422},
  year={1973},
  publisher={IOP Publishing}
}


@article{lauterbur1973image,
  title={Image formation by induced local interactions: examples employing nuclear magnetic resonance},
  author={Lauterbur, Paul C},
  journal={nature},
  volume={242},
  number={5394},
  pages={190--191},
  year={1973},
  publisher={Nature Publishing Group}
}


@unpublished{sanchez2018master,
  title        ={Learning-Based Non-Cartesian Compressed Sensing for Dynamic {MRI}},
  author       ={Sanchez,Thomas},
  note         ={Master Thesis},
  year         ={2018},
}
@book{brown2014magnetic,
  title={Magnetic resonance imaging: physical principles and sequence design},
  author={Brown, Robert W and Cheng, Y-C Norman and Haacke, E Mark and Thompson, Michael R and Venkatesan, Ramesh},
  year={2014},
  publisher={John Wiley \& Sons}
}
@article{kazeminia2020gans,
  title={GANs for medical image analysis},
  author={Kazeminia, Salome and Baur, Christoph and Kuijper, Arjan and van Ginneken, Bram and Navab, Nassir and Albarqouni, Shadi and Mukhopadhyay, Anirban},
  journal={Artificial Intelligence in Medicine},
  volume={109},
  pages={101938},
  year={2020},
  publisher={Elsevier}
}

@article{pan2019recent,
  title={Recent progress on generative adversarial networks (GANs): A survey},
  author={Pan, Zhaoqing and Yu, Weijie and Yi, Xiaokai and Khan, Asifullah and Yuan, Feng and Zheng, Yuhui},
  journal={IEEE Access},
  volume={7},
  pages={36322--36333},
  year={2019},
  publisher={IEEE}
}


@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}


@article{zhang2021conditional,
  title={Conditional Variational Autoencoder for Learned Image Reconstruction},
  author={Zhang, Chen and Barbano, Riccardo and Jin, Bangti},
  journal={Computation},
  volume={9},
  number={11},
  pages={114},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}


@article{song2021solving,
  title={Solving Inverse Problems in Medical Imaging with Score-Based Generative Models},
  author={Song, Yang and Shen, Liyue and Xing, Lei and Ermon, Stefano},
  journal={arXiv preprint arXiv:2111.08005},
  year={2021}
}


@article{sun2020deep,
  title={Deep probabilistic imaging: Uncertainty quantification and multi-modal solution characterization for computational imaging},
  author={Sun, He and Bouman, Katherine L},
  journal={arXiv preprint arXiv:2010.14462},
  volume={9},
  year={2020}
}


@article{oord2016conditional,
  title={Conditional image generation with pixelcnn decoders},
  author={Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1606.05328},
  year={2016}
}


@inproceedings{van2016pixel,
  title={Pixel recurrent neural networks},
  author={Van Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1747--1756},
  year={2016},
  organization={PMLR}
}


@article{papamakarios2019normalizing,
  title={Normalizing flows for probabilistic modeling and inference},
  author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02762},
  year={2019}
}


@article{denker2021conditional,
  title={Conditional Invertible Neural Networks for Medical Imaging},
  author={Denker, Alexander and Schmidt, Maximilian and Leuschner, Johannes and Maass, Peter},
  journal={Journal of Imaging},
  volume={7},
  number={11},
  pages={243},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{zhu2017multimodal,
  title={Multimodal Image-to-Image Translation by Enforcing Bi-Cycle Consistency},
  author={Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A and Wang, Oliver and Shechtman, Eli},
  booktitle={Advances in neural information processing systems},
  pages={465--476},
  year={2017}
}


@article{goodfellow2016nips,
  title={Nips 2016 tutorial: Generative adversarial networks},
  author={Goodfellow, Ian},
  journal={arXiv preprint arXiv:1701.00160},
  year={2016}
}


@article{edupuganti2020uncertainty,
  title={Uncertainty quantification in deep mri reconstruction},
  author={Edupuganti, Vineet and Mardani, Morteza and Vasanawala, Shreyas and Pauly, John},
  journal={IEEE Transactions on Medical Imaging},
  volume={40},
  number={1},
  pages={239--250},
  year={2020},
  publisher={IEEE}
}


@article{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  pages={2613--2621},
  year={2010}
}


 @article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

 @article{monahan1982state,
  title={State of the art—a survey of partially observable Markov decision processes: theory, models, and algorithms},
  author={Monahan, George E},
  journal={Management science},
  volume={28},
  number={1},
  pages={1--16},
  year={1982},
  publisher={INFORMS}
}


 @misc{weng2018a, 
    title={A (Long) Peek into Reinforcement Learning}, 
    url={https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html}, 
    author={Weng, Lilian}, 
    year={2018}, 
    month={Feb},
    note={https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html}
    } 

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}


@inproceedings{coulom2006efficient,
  title={Efficient selectivity and backup operators in Monte-Carlo tree search},
  author={Coulom, R{\'e}mi},
  booktitle={International conference on computers and games},
  pages={72--83},
  year={2006},
  organization={Springer}
}


@incollection{rauhut2010compressive,
  title={Compressive sensing and structured random matrices},
  author={Rauhut, Holger},
  booktitle={Theoretical foundations and numerical methods for sparse recovery},
  pages={1--92},
  year={2010},
  publisher={de Gruyter}
}


@article{bakker2021learning,
  title={On learning adaptive acquisition policies for undersampled multi-coil MRI reconstruction},
  author={Bakker, Tim and Muckley, Matthew J and Romero-Soriano, Adriana and Drozdzal, Michal and Pineda, Luis},
  year={2021}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}


@article{van2019use,
  title={When to use parametric models in reinforcement learning?},
  author={van Hasselt, Hado P and Hessel, Matteo and Aslanides, John},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={14322--14333},
  year={2019}
}

@article{fritz2016six,
  title={Six-fold acceleration of high-spatial resolution 3D SPACE MRI of the knee through incoherent k-space undersampling and iterative reconstruction—first experience},
  author={Fritz, Jan and Raithel, Esther and Thawait, Gaurav K and Gilson, Wesley and Papp, Derek F},
  journal={Investigative radiology},
  volume={51},
  number={6},
  pages={400--409},
  year={2016},
  publisher={LWW}
}

@inproceedings{sawyer2013creation,
  title={Creation of fully sampled MR data repository for compressed sensing of the knee},
  author={Sawyer, Anne Marie and Lustig, Michael and Alley, Marcus and Uecker, Phdmartin and Virtue, Patrick and Lai, Peng and Vasanawala, Shreyas},
  year={2013},
  booktitle={Proceedings of the 22nd Annual Meeting for Section for Magnetic Resonance Technologists, Salt Lake City, Utah, USA},
}


@article{boyer2017compressed,
  title={Compressed sensing with structured sparsity and structured acquisition},
  author={Boyer, Claire and Bigot, J{\'e}r{\'e}mie and Weiss, Pierre},
  journal={Applied and Computational Harmonic Analysis},
  year={2017},
  publisher={Elsevier}
}

@misc{krause2014submodular,
  title={Submodular function maximization.},
  author={Krause, Andreas and Golovin, Daniel},
  year={2014}
}

@inproceedings{bridson2007fast,
  title={Fast Poisson disk sampling in arbitrary dimensions.},
  author={Bridson, Robert},
  booktitle={SIGGRAPH sketches},
  pages={22},
  year={2007}
}

@article{breuer2006controlled,
  title={Controlled aliasing in volumetric parallel imaging {(2D CAIPIRINHA)}},
  author={Breuer, Felix A and Blaimer, Martin and Mueller, Matthias F and Seiberlich, Nicole and Heidemann, Robin M and Griswold, Mark A and Jakob, Peter M},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={55},
  number={3},
  pages={549--556},
  year={2006},
  publisher={Wiley Online Library}
}

@Article{kutyniok2016shearlab,
  Title                    = {Shearlab {3D}: Faithful digital shearlet transforms based on compactly supported shearlets},
  Author                   = {Kutyniok, Gitta and Lim, Wang-Q and Reisenhofer, Rafael},
  Journal                  = {ACM Trans. Math. Softw.},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {5},
  Volume                   = {42},

  Publisher                = {ACM}
}

@article{nemhauser1978analysis,
  title={An analysis of approximations for maximizing submodular set functions {I}},
  author={Nemhauser, George L and Wolsey, Laurence A and Fisher, Marshall L},
  journal={Mathematical programming},
  volume={14},
  number={1},
  pages={265--294},
  year={1978},
  publisher={Springer}
}

@incollection{minoux1978accelerated,
  title={Accelerated greedy algorithms for maximizing submodular set functions},
  author={Minoux, Michel},
  booktitle={Optimization techniques},
  pages={234--243},
  year={1978},
  publisher={Springer}
}

@article{adcock2018oracle,
  title={On oracle-type local recovery guarantees in compressed sensing},
  author={Adcock, Ben and Boyer, Claire and Brugiapaglia, Simone},
  journal={arXiv preprint arXiv:1806.03789},
  year={2018}
}

@article{lazarus2018variable,
  title={Variable-density k-space filling curves for accelerated Magnetic Resonance Imaging},
  author={Lazarus, Carole and Weiss, P and Chauffert, N and Mauconduit, F and El Gueddari, Loubna and Destrieux, C and Zemmoura, I and Vignaud, A and Ciuciu, Philippe},
  year={2018}
}

@article{chaari2011wavelet,
  title={A wavelet-based regularized reconstruction algorithm for {SENSE} parallel {MRI} with applications to neuroimaging},
  author={Cha{\^a}ri, Lotfi and Pesquet, Jean-Christophe and Benazza-Benyahia, Amel and Ciuciu, Philippe},
  journal={Medical image analysis},
  volume={15},
  number={2},
  pages={185--201},
  year={2011},
  publisher={Elsevier}
}

@article{uecker2014espirit,
  title={{ESPIRiT}: an eigenvalue approach to autocalibrating parallel {MRI}: where {SENSE} meets {GRAPPA}},
  author={Uecker, Martin and Lai, Peng and Murphy, Mark J and Virtue, Patrick and Elad, Michael and Pauly, John M and Vasanawala, Shreyas S and Lustig, Michael},
  journal={Magn. Reson. Med.},
  volume={71},
  number={3},
  pages={990--1001},
  year={2014},
  publisher={Wiley Online Library}
}

@inproceedings{uecker2015berkeley,
  title={Berkeley advanced reconstruction toolbox},
  author={Uecker, Martin and Ong, Frank and Tamir, Jonathan I and Bahri, Dara and Virtue, Patrick and Cheng, Joseph Y and Zhang, Tao and Lustig, Michael},
  booktitle={Proc. Intl. Soc. Mag. Reson. Med},
  volume={23},
  pages={2486},
  year={2015}
}

@article{fessler2018limitations,
  title={Limitations \& caveats of deep learning},
  author={Fessler, Jeffrey A},
  journal={ISMRM course on Deep Learning},
  year={2018}
}

@article{baraniuk2010model,
  title={Model-based compressive sensing},
  author={Baraniuk, Richard G and Cevher, Volkan and Duarte, Marco F and Hegde, Chinmay},
  journal={IEEE Transactions on information theory},
  volume={56},
  number={4},
  pages={1982--2001},
  year={2010},
  publisher={IEEE}
}


@misc{sanchez2021on,
  bibtex_show = {true},
  abbr = {Preprint},
  author = {Sanchez, Thomas and Krawczuk, Igor and Cevher, Volkan},
  title = {On the benefits of deep {RL} in accelerated {MRI} sampling},
  year = {2021},
  note = {\textit{Available at \url{https://openreview.net/pdf?id=fRb9LBWUo56}}},
  selected = {true}
}



@article{marseille1996nonuniform,
  title={Nonuniform phase-encode distributions for MRI scan time reduction},
  author={Marseille, GJ and De Beer, R and Fuderer, M and Mehlkopf, AF and Van Ormondt, D},
  journal={Journal of Magnetic Resonance, Series B},
  volume={1},
  number={111},
  pages={70--75},
  year={1996}
}


@article{tsai2000reduced,
  title={Reduced aliasing artifacts using variable-density k-space sampling trajectories},
  author={Tsai, Chi-Ming and Nishimura, Dwight G},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={43},
  number={3},
  pages={452--458},
  year={2000},
  publisher={Wiley Online Library}
}

@article{candes2007sparsity,
  title={Sparsity and incoherence in compressive sampling},
  author={Candes, Emmanuel and Romberg, Justin},
  journal={Inverse problems},
  volume={23},
  number={3},
  pages={969},
  year={2007},
  publisher={IOP Publishing}
}

@article{krahmer2013stable,
  title={Stable and robust sampling strategies for compressive imaging},
  author={Krahmer, Felix and Ward, Rachel},
  journal={IEEE transactions on image processing},
  volume={23},
  number={2},
  pages={612--622},
  year={2013},
  publisher={IEEE}
}


@inproceedings{seeger2008compressed,
  title={Compressed sensing and Bayesian experimental design},
  author={Seeger, Matthias W and Nickisch, Hannes},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={912--919},
  year={2008}
}

@inproceedings{seeger2007bayesian,
  title={Bayesian inference and optimal design in the sparse linear model},
  author={Seeger, Matthias and Steinke, Florian and Tsuda, Koji},
  booktitle={Artificial Intelligence and Statistics},
  pages={444--451},
  year={2007},
  organization={PMLR}
}


@article{tsao2005optimizing,
  title={Optimizing spatiotemporal sampling for k-t BLAST and k-t SENSE: application to high-resolution real-time cardiac steady-state free precession},
  author={Tsao, Jeffrey and Kozerke, Sebastian and Boesiger, Peter and Pruessmann, Klaas P},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={53},
  number={6},
  pages={1372--1382},
  year={2005},
  publisher={Wiley Online Library}
}

@article{candes2008introduction,
  title={An introduction to compressive sampling},
  author={Cand{\`e}s, Emmanuel J and Wakin, Michael B},
  journal={IEEE signal processing magazine},
  volume={25},
  number={2},
  pages={21--30},
  year={2008},
  publisher={IEEE}
}

@article{rueckert2019model,
  title={Model-based and data-driven strategies in medical image computing},
  author={Rueckert, Daniel and Schnabel, Julia A},
  journal={Proceedings of the IEEE},
  volume={108},
  number={1},
  pages={110--124},
  year={2019},
  publisher={IEEE}
}


@article{zijlstra2016evaluation,
  title={Evaluation of variable density and data-driven k-space undersampling for compressed sensing magnetic resonance imaging},
  author={Zijlstra, Frank and Viergever, Max A and Seevinck, Peter R},
  journal={Investigative radiology},
  volume={51},
  number={6},
  pages={410--419},
  year={2016},
  publisher={LWW}
}


@incollection{ciancarella2014adaptive,
  title={Adaptive sampling and reconstruction for sparse magnetic resonance imaging},
  author={Ciancarella, Laura and Avola, Danilo and Placidi, Giuseppe},
  booktitle={Computational Modeling of Objects Presented in Images},
  pages={115--130},
  year={2014},
  publisher={Springer}
}


@inproceedings{seeger2009bayesian,
  title={Bayesian experimental design of magnetic resonance imaging sequences},
  author={Seeger, Matthias and Nickisch, Hannes and Pohmann, Rolf and Sch{\"o}lkopf, Bernhard},
  booktitle={Proceedings of the 22nd Annual Conference on Neural Information Processing Systems},
  number={CONF},
  pages={1441--1448},
  year={2009}
}


@inproceedings{weiss2007learning,
  title={Learning compressed sensing},
  author={Weiss, Yair and Chang, Hyun Sung and Freeman, William T},
  booktitle={Snowbird Learning Workshop, Allerton, CA},
  year={2007},
  organization={Citeseer}
}


@article{puy2011variable,
  title={On variable density compressive sampling},
  author={Puy, Gilles and Vandergheynst, Pierre and Wiaux, Yves},
  journal={IEEE signal processing letters},
  volume={18},
  number={10},
  pages={595--598},
  year={2011},
  publisher={IEEE}
}



@article{darestani2021measuring,
  title={Measuring Robustness in Deep Learning Based Compressive Sensing},
  author={Darestani, Mohammad Zalbagi and Chaudhari, Akshay and Heckel, Reinhard},
  journal={arXiv preprint arXiv:2102.06103},
  year={2021}
}


@article{ravishankar2017efficient,
  title={Efficient sum of outer products dictionary learning (SOUP-DIL) and its application to inverse problems},
  author={Ravishankar, Saiprasad and Nadakuditi, Raj Rao and Fessler, Jeffrey A},
  journal={IEEE transactions on computational imaging},
  volume={3},
  number={4},
  pages={694--709},
  year={2017},
  publisher={IEEE}
}


@article{ravishankar2019image,
  title={Image reconstruction: From sparsity to data-adaptive methods and machine learning},
  author={Ravishankar, Saiprasad and Ye, Jong Chul and Fessler, Jeffrey A},
  journal={Proceedings of the IEEE},
  volume={108},
  number={1},
  pages={86--109},
  year={2019},
  publisher={IEEE}
}

@article{doneva2020mathematical,
  title={Mathematical models for magnetic resonance imaging reconstruction: An overview of the approaches, problems, and future research areas},
  author={Doneva, Mariya},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={1},
  pages={24--32},
  year={2020},
  publisher={IEEE}
}


@article{lucas2018using,
  title={Using deep neural networks for inverse problems in imaging: beyond analytical methods},
  author={Lucas, Alice and Iliadis, Michael and Molina, Rafael and Katsaggelos, Aggelos K},
  journal={IEEE Signal Processing Magazine},
  volume={35},
  number={1},
  pages={20--36},
  year={2018},
  publisher={IEEE}
}


@inproceedings{yaman2020self,
  title={Self-supervised physics-based deep learning MRI reconstruction without fully-sampled data},
  author={Yaman, Burhaneddin and Hosseini, Seyed Amir Hossein and Moeller, Steen and Ellermann, Jutta and U{\u{g}}urbil, K{\^a}mil and Ak{\c{c}}akaya, Mehmet},
  booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},
  pages={921--925},
  year={2020},
  organization={IEEE}
}


@inproceedings{wang2003multiscale,
  title={Multiscale structural similarity for image quality assessment},
  author={Wang, Zhou and Simoncelli, Eero P and Bovik, Alan C},
  booktitle={The Thrity-Seventh Asilomar Conference on Signals, Systems \& Computers, 2003},
  volume={2},
  pages={1398--1402},
  year={2003},
  organization={Ieee}
}


@article{zhao2016loss,
  title={Loss functions for image restoration with neural networks},
  author={Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
  journal={IEEE Transactions on computational imaging},
  volume={3},
  number={1},
  pages={47--57},
  year={2016},
  publisher={IEEE}
}


@incollection{combettes2011proximal,
  title={Proximal splitting methods in signal processing},
  author={Combettes, Patrick L and Pesquet, Jean-Christophe},
  booktitle={Fixed-point algorithms for inverse problems in science and engineering},
  pages={185--212},
  year={2011},
  publisher={Springer}
}

@article{daubechies2004iterative,
  title={An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
  author={Daubechies, Ingrid and Defrise, Michel and De Mol, Christine},
  journal={Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
  volume={57},
  number={11},
  pages={1413--1457},
  year={2004},
  publisher={Wiley Online Library}
}



@article{yoo2021time,
  title={Time-Dependent Deep Image Prior for Dynamic MRI},
  author={Yoo, Jaejun and Jin, Kyong Hwan and Gupta, Harshit and Yerly, Jerome and Stuber, Matthias and Unser, Michael},
  journal={IEEE Transactions on Medical Imaging},
  year={2021},
  publisher={IEEE}
}

@inproceedings{ulyanov2018deep,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9446--9454},
  year={2018}
}


@article{patel2021gan,
  title={GAN-Based Priors for Quantifying Uncertainty in Supervised Learning},
  author={Patel, Dhruv V and Oberai, Assad A},
  journal={SIAM/ASA Journal on Uncertainty Quantification},
  volume={9},
  number={3},
  pages={1314--1343},
  year={2021},
  publisher={SIAM}
}


@article{jalal2021robust,
  title={Robust Compressed Sensing MRI with Deep Generative Priors},
  author={Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G and Tamir, Jonathan I},
  journal={arXiv preprint arXiv:2108.01368},
  year={2021}
}


@article{ravishankar2010mr,
  title={MR image reconstruction from highly undersampled k-space data by dictionary learning},
  author={Ravishankar, Saiprasad and Bresler, Yoram},
  journal={IEEE transactions on medical imaging},
  volume={30},
  number={5},
  pages={1028--1041},
  year={2010},
  publisher={IEEE}
}


@article{darestani2021accelerated,
  title={Accelerated MRI with un-trained neural networks},
  author={Darestani, Mohammad Zalbagi and Heckel, Reinhard},
  journal={IEEE Transactions on Computational Imaging},
  volume={7},
  pages={724--733},
  year={2021},
  publisher={IEEE}
}


@article{liang2020deep,
  title={Deep magnetic resonance image reconstruction: Inverse problems meet neural networks},
  author={Liang, Dong and Cheng, Jing and Ke, Ziwen and Ying, Leslie},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={1},
  pages={141--151},
  year={2020},
  publisher={IEEE}
}


@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{Engstrom2020Implementation,
title={Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1etN1rtPB},
}

@inproceedings{van2021active,
  title={Active Deep Probabilistic Subsampling},
  author={Van Gorp, Hans and Huijben, Iris and Veeling, Bastiaan S and Pezzotti, Nicola and Van Sloun, Ruud JG},
  booktitle={International Conference on Machine Learning},
  pages={10509--10518},
  year={2021},
  organization={PMLR}
}

@article{shimron2021subtle,
  title={Subtle Inverse Crimes: Naively training machine learning algorithms could lead to overly-optimistic results},
  author={Shimron, Efrat and Tamir, Jonathan I and Wang, Ke and Lustig, Michael},
  journal={arXiv preprint arXiv:2109.08237},
  year={2021}
}

@inproceedings{adcock2017breaking,
  title={Breaking the coherence barrier: {A} new theory for compressed sensing},
  author={Adcock, Ben and Hansen, Anders C and Poon, Clarice and Roman, Bogdan},
  booktitle={Forum of Mathematics, Sigma},
  volume={5},
  year={2017},
  organization={Cambridge University Press}
}

@inproceedings{deepRL2018,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@inproceedings{ingredientsdeeppolicy,
  title     = {The Successful Ingredients of Policy Gradient Algorithms},
  author    = {Gronauer, Sven and Gottwald, Martin and Diepold, Klaus},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {2455--2461},
  year      = {2021},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2021/338},
  url       = {https://doi.org/10.24963/ijcai.2021/338},
}

@article{lazarus2019sparkling,
  title={SPARKLING: variable-density k-space filling curves for accelerated T2*-weighted MRI},
  author={Lazarus, Carole and Weiss, Pierre and Chauffert, Nicolas and Mauconduit, Franck and El Gueddari, Loubna and Destrieux, Christophe and Zemmoura, Ilyess and Vignaud, Alexandre and Ciuciu, Philippe},
  journal={Magnetic resonance in medicine},
  volume={81},
  number={6},
  pages={3643--3661},
  year={2019},
  publisher={Wiley Online Library},
}

%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/




%% Created for Matthias Geissbühler at 2011-03-21 13:46:46 +0100 
@incollection{latorre2019fast,
title = {Fast and Provable {ADMM} for Learning with Generative Priors},
author = {Latorre, Fabian and Eftekhari, Armin and Cevher, Volkan},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {12027--12039},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9371-fast-and-provable-admm-for-learning-with-generative-priors.pdf}
}
@article{bora2017compressed,
  title={Compressed sensing using generative models},
  author={Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G},
  journal={arXiv preprint arXiv:1703.03208},
  year={2017}
}

@article{pineda2020active,
  title={Active MR k-space Sampling with Reinforcement Learning},
  author={Pineda, Luis and Basu, Sumana and Romero, Adriana and Calandra, Roberto and Drozdzal, Michal},
  journal={arXiv preprint arXiv:2007.10469},
  year={2020}
}


@article{huijben2020ultrasound,
  title={Learning sub-sampling and signal recovery with applications in ultrasound imaging},
  author={Huijben, Iris AM and Veeling, Bastiaan S and Janse, Kees and Mischi, Massimo and Van Sloun, Ruud JG},
  journal={IEEE Transactions on Medical Imaging},
  year={2020},
  publisher={IEEE}
}

@misc{
sanchez2020closed,
title={Closed loop deep {Bayesian} inversion:  Uncertainty driven acquisition for fast {MRI}},
author={Thomas Sanchez and Igor Krawczuk and Zhaodong Sun and Volkan Cevher},
year={2020},
note={\textit{Available at \url{https://openreview.net/pdf?id=BJlPOlBKDB}}}
}

@article{kessler2020deep,
  title={Deep-Learning Based Adaptive Ultrasound Imaging from Sub-Nyquist Channel Data},
  author={Kessler, Naama and Eldar, Yonina C},
  journal={arXiv preprint arXiv:2008.02628},
  year={2020}
}


@article{rivenson2018phase,
  title={Phase recovery and holographic image reconstruction using deep learning in neural networks},
  author={Rivenson, Yair and Zhang, Yibo and G{\"u}nayd{\i}n, Harun and Teng, Da and Ozcan, Aydogan},
  journal={Light: Science \& Applications},
  volume={7},
  number={2},
  pages={17141--17141},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{jin2017deep,
  title={Deep convolutional neural network for inverse problems in imaging},
  author={Jin, Kyong Hwan and McCann, Michael T and Froustey, Emmanuel and Unser, Michael},
  journal={IEEE Transactions on Image Processing},
  volume={26},
  number={9},
  pages={4509--4522},
  year={2017},
  publisher={IEEE}
}


@InProceedings{wu2019learning,
  title = 	 {Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling},
  author = 	 {Wu, Shanshan and Dimakis, Alex and Sanghavi, Sujay and Yu, Felix and Holtmann-Rice, Daniel and Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6828--6839},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wu19b/wu19b.pdf},
  url = 	 {http://proceedings.mlr.press/v97/wu19b.html},
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015}
}


@book{fornasini2008uncertainty,
  title={The uncertainty in physical measurements: an introduction to data analysis in the physics laboratory},
  author={Fornasini, Paolo},
  year={2008},
  publisher={Springer Science \& Business Media}
}


@inproceedings{sohn2015learning,
  title={Learning structured output representation using deep conditional generative models},
  author={Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  booktitle={Advances in neural information processing systems},
  pages={3483--3491},
  year={2015}
}

@article{dashti2013bayesian,
  title={The Bayesian approach to inverse problems},
  author={Dashti, Masoumeh and Stuart, Andrew M},
  journal={arXiv preprint arXiv:1302.6989},
  year={2013}
}


@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{rezende2014stochastic,
  title={Stochastic backpropagation and approximate inference in deep generative models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  journal={arXiv preprint arXiv:1401.4082},
  year={2014}
}

@inproceedings{foster2019variational,
  title={Variational Bayesian Optimal Experimental Design},
  author={Foster, Adam and Jankowiak, Martin and Bingham, Elias and Horsfall, Paul and Teh, Yee Whye and Rainforth, Thomas and Goodman, Noah},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14036--14047},
  year={2019}
}

@article{attia2018goal,
  title={Goal-oriented optimal design of experiments for large-scale Bayesian linear inverse problems},
  author={Attia, Ahmed and Alexanderian, Alen and Saibaba, Arvind K},
  journal={Inverse Problems},
  volume={34},
  number={9},
  pages={095009},
  year={2018},
  publisher={IOP Publishing}
}



@InProceedings{wu2019deep,
  title = 	 {Deep Compressed Sensing},
  author = 	 {Wu, Yan and Rosca, Mihaela and Lillicrap, Timothy},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6850--6860},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wu19d/wu19d.pdf},
  url = 	 {http://proceedings.mlr.press/v97/wu19d.html},
}


@article{babacan2009bayesian,
  title={Bayesian compressive sensing using Laplace priors},
  author={Babacan, S Derin and Molina, Rafael and Katsaggelos, Aggelos K},
  journal={IEEE Transactions on image processing},
  volume={19},
  number={1},
  pages={53--63},
  year={2009},
  publisher={IEEE}
}


@article{shewry1987maximum,
  title={Maximum entropy sampling},
  author={Shewry, Michael C and Wynn, Henry P},
  journal={Journal of applied statistics},
  volume={14},
  number={2},
  pages={165--170},
  year={1987},
  publisher={Taylor \& Francis}
}


@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}


@book{cover2012elements,
  title={Elements of information theory},
  author={Cover, Thomas M and Thomas, Joy A},
  year={2012},
  publisher={John Wiley \& Sons}
}


@INPROCEEDINGS{huijben2020learning,
  author={I. A. M. {Huijben} and B. S. {Veeling} and R. J. G. {van Sloun}},
  booktitle={2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Learning Sampling and Model-Based Signal Recovery for Compressed Sensing MRI}, 
  year={2020},
  volume={},
  number={},
  pages={8906-8910},}


@article{tonolini2019variational,
  title={Variational inference for computational imagsing inverse problems},
  author={Tonolini, Francesco and Lyons, Ashley and Caramazza, Piergiorgio and Faccio, Daniele and Murray-Smith, Roderick},
  journal={arXiv preprint arXiv:1904.06264},
  year={2019}
}


@article{devries2019evaluation,
  title={On the evaluation of conditional gans},
  author={DeVries, Terrance and Romero, Adriana and Pineda, Luis and Taylor, Graham W and Drozdzal, Michal},
  journal={arXiv preprint arXiv:1907.08175},
  year={2019}
}


@misc{ravuri2019classification,
    title={Classification Accuracy Score for Conditional Generative Models},
    author={Suman Ravuri and Oriol Vinyals},
    year={2019},
    eprint={1905.10887},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{ji2008bayesian,
  title={Bayesian compressive sensing},
  author={Ji, Shihao and Xue, Ya and Carin, Lawrence},
  journal={IEEE Transactions on signal processing},
  volume={56},
  number={6},
  pages={2346--2356},
  year={2008},
  publisher={IEEE}
}

@inproceedings{haupt2009adaptive,
  title={Adaptive sensing for sparse signal recovery},
  author={Haupt, Jarvis and Nowak, Robert and Castro, Rui},
  booktitle={2009 IEEE 13th Digital Signal Processing Workshop and 5th IEEE Signal Processing Education Workshop},
  pages={702--707},
  year={2009},
  organization={IEEE}
}


@article{chaloner1995bayesian,
  title={Bayesian experimental design: A review},
  author={Chaloner, Kathryn and Verdinelli, Isabella},
  journal={Statistical Science},
  pages={273--304},
  year={1995},
  publisher={JSTOR}
}


@article{lindley1956measure,
  title={On a measure of the information provided by an experiment},
  author={Lindley, Dennis V},
  journal={The Annals of Mathematical Statistics},
  pages={986--1005},
  year={1956},
  publisher={JSTOR}
}


@article{vanlier2012bayesian,
  title={A Bayesian approach to targeted experiment design},
  author={Vanlier, Joep and Tiemann, Christian A and Hilbers, Peter AJ and van Riel, Natal AW},
  journal={Bioinformatics},
  volume={28},
  number={8},
  pages={1136--1142},
  year={2012},
  publisher={Oxford University Press}
}


@article{huan2016sequential,
  title={Sequential Bayesian optimal experimental design via approximate dynamic programming},
  author={Huan, Xun and Marzouk, Youssef M},
  journal={arXiv preprint arXiv:1604.08320},
  year={2016}
}


@article{jaspan2015compressed,
  title={Compressed sensing MRI: a review of the clinical literature},
  author={Jaspan, Oren N and Fleysher, Roman and Lipton, Michael L},
  journal={The British journal of radiology},
  volume={88},
  number={1056},
  pages={20150487},
  year={2015},
  publisher={The British Institute of Radiology.}
}

@article{pauwels2014bayesian,
  title={A Bayesian active learning strategy for sequential experimental design in systems biology},
  author={Pauwels, Edouard and Lajaunie, Christian and Vert, Jean-Philippe},
  journal={BMC systems biology},
  volume={8},
  number={1},
  pages={102},
  year={2014},
  publisher={BioMed Central}
}

@inproceedings{
ivanov2018variational,
title={Variational Autoencoder with Arbitrary Conditioning},
author={Oleg Ivanov and Michael Figurnov and Dmitry Vetrov},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SyxtJh0qYm},
}


@article{lindgren2020conditional,
  title={Conditional Sampling from Invertible Generative Models with Applications to Inverse Problems},
  author={Lindgren, Erik M and Whang, Jay and Dimakis, Alexandros G},
  journal={arXiv preprint arXiv:2002.11743},
  year={2020}
}


@article{malinen2005scanning,
  title={Scanning path optimization for ultrasound surgery},
  author={Malinen, Matti and Huttunen, Tomi and Kaipio, Jari P and Hynynen, Kullervo},
  journal={Physics in Medicine \& Biology},
  volume={50},
  number={15},
  pages={3473},
  year={2005},
  publisher={IOP Publishing}
}
@inproceedings{belghazi2019learning,
title = {Learning about an exponential amount of conditional distributions},
author = {Belghazi, Mohamed and Oquab, Maxime and Lopez-Paz, David},
booktitle = {Advances in Neural Information Processing Systems 32},
year = {2019},
url = {http://papers.nips.cc/paper/9523-learning-about-an-exponential-amount-of-conditional-distributions.pdf}
}


@inproceedings{Huijben2020Deep,
title={Deep probabilistic subsampling for task-adaptive compressed sensing},
author={Iris A.M. Huijben and Bastiaan S. Veeling and Ruud J.G. van Sloun},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJeq9JBFvH}
}



@article{kovarik2016implementing,
  title={Implementing an accurate and rapid sparse sampling approach for low-dose atomic resolution STEM imaging},
  author={Kovarik, Libor and Stevens, A and Liyu, A and Browning, Nigel D},
  journal={Applied Physics Letters},
  volume={109},
  number={16},
  pages={164102},
  year={2016},
  publisher={AIP Publishing}
}

@article{levine2017fly,
  title={On-the-Fly Adaptive $k$-Space Sampling for Linear MRI Reconstruction Using Moment-Based Spectral Analysis},
  author={Levine, Evan and Hargreaves, Brian},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={2},
  pages={557--567},
  year={2017},
  publisher={IEEE}
}

@article{han2019k,
  title={k-space deep learning for accelerated MRI},
  author={Han, Yoseob and Sunwoo, Leonard and Ye, Jong Chul},
  journal={IEEE transactions on medical imaging},
  year={2019},
  publisher={IEEE}
}

@inproceedings{lustig2005faster,
  title={Faster imaging with randomly perturbed, under-sampled spirals and l1 reconstruction},
  author={Lustig, Michael and Lee, Jin Hyung and Donoho, David L and Pauly, John M},
  booktitle={Proceedings of the 13th annual meeting of ISMRM, Miami Beach},
  pages={685},
  year={2005},
  organization={Citeseer}
}
@inproceedings{abramovitch2007tutorial,
  title={A tutorial on the mechanisms, dynamics, and control of atomic force microscopes},
  author={Abramovitch, Daniel Y and Andersson, Sean B and Pao, Lucy Y and Schitter, Georg},
  booktitle={2007 American Control Conference},
  pages={3488--3502},
  year={2007},
  organization={IEEE}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}
@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@book{park2018fundamentals,
  title={Fundamentals of Probability and Stochastic Processes with Applications to Communications},
  author={Park, Kun Il and Park},
  year={2018},
  publisher={Springer}
}




@inproceedings{schlemper2017deep,
  title={A deep cascade of convolutional neural networks for MR image reconstruction},
  author={Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V and Price, Anthony and Rueckert, Daniel},
  booktitle={International Conference on Information Processing in Medical Imaging},
  pages={647--658},
  year={2017},
  organization={Springer}
}

@inproceedings{sun2016deep,
  title={Deep ADMM-Net for compressive sensing MRI},
  author={Sun, Jian and Li, Huibin and Xu, Zongben and others},
  booktitle={Advances in neural information processing systems},
  pages={10--18},
  year={2016}
}

@inproceedings{wang2016accelerating,
  title={Accelerating magnetic resonance imaging via deep learning},
  author={Wang, Shanshan and Su, Zhenghang and Ying, Leslie and Peng, Xi and Zhu, Shun and Liang, Feng and Feng, Dagan and Liang, Dong},
  booktitle={2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)},
  pages={514--517},
  year={2016},
  organization={IEEE}
}

@article{kang2017deep,
  title={A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction},
  author={Kang, Eunhee and Min, Junhong and Ye, Jong Chul},
  journal={Medical physics},
  volume={44},
  number={10},
  pages={e360--e375},
  year={2017},
  publisher={Wiley Online Library}
}

@inproceedings{vishnevskiy2018image,
  title={Image reconstruction via variational network for real-time hand-held sound-speed imaging},
  author={Vishnevskiy, Valery and Sanabria, Sergio J and Goksel, Orcun},
  booktitle={International Workshop on Machine Learning for Medical Image Reconstruction},
  pages={120--128},
  year={2018},
  organization={Springer}
}


@article{rajpurkar2017mura,
  title={Mura: Large dataset for abnormality detection in musculoskeletal radiographs},
  author={Rajpurkar, Pranav and Irvin, Jeremy and Bagul, Aarti and Ding, Daisy and Duan, Tony and Mehta, Hershel and Yang, Brandon and Zhu, Kaylie and Laird, Dillon and Ball, Robyn L and others},
  journal={arXiv preprint arXiv:1712.06957},
  year={2017}
}

@article{kamnitsas2017efficient,
  title={Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation},
  author={Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia FJ and Simpson, Joanna P and Kane, Andrew D and Menon, David K and Rueckert, Daniel and Glocker, Ben},
  journal={Medical image analysis},
  volume={36},
  pages={61--78},
  year={2017},
  publisher={Elsevier}
}

@article{bai2017human,
  title={Human-level CMR image analysis with deep fully convolutional networks},
  author={Bai, Wenjia and Sinclair, Matthew and Tarroni, Giacomo and Oktay, Ozan and Rajchl, Martin and Vaillant, Ghislain and Lee, Aaron M and Aung, Nay and Lukaschuk, Elena and Sanghvi, Mihir M and others},
  year={2017}
}

@inproceedings{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in neural information processing systems},
  pages={5574--5584},
  year={2017}
}

@article{kaelbling1998planning,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1-2},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{bahadir2019learning,
  title={Learning-based Optimization of the Under-sampling Pattern in {MRI}},
  author={Bahadir, Cagla Deniz and Dalca, Adrian V and Sabuncu, Mert R},
  booktitle={International Conference on Information Processing in Medical Imaging},
  pages={780--792},
  year={2019},
  organization={Springer}
}


@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1321--1330},
  year={2017},
  organization={JMLR. org}
}

@article{bydder2002combination,
  title={Combination of signals from array coils using image-based estimation of coil sensitivity profiles},
  author={Bydder, Mark and Larkman, David J and Hajnal, Joseph V},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={47},
  number={3},
  pages={539--548},
  year={2002},
  publisher={Wiley Online Library}
}

@inproceedings{adler2018banach,
  title={Banach wasserstein gan},
  author={Adler, Jonas and Lunz, Sebastian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6754--6763},
  year={2018}
}

@inproceedings{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in neural information processing systems},
  pages={5767--5777},
  year={2017}
}

@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{arjovsky2017wasserstein,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

@inproceedings{chauffert2013variable,
  title={Variable density compressed sensing in {MRI}. Theoretical vs heuristic sampling strategies},
  author={Chauffert, Nicolas and Ciuciu, Philippe and Weiss, Pierre},
  booktitle={2013 IEEE 10th International Symposium on Biomedical Imaging},
  pages={298--301},
  year={2013},
  organization={IEEE}
}

@article{chauffert2014variable,
  title={Variable density sampling with continuous trajectories},
  author={Chauffert, Nicolas and Ciuciu, Philippe and Kahn, Jonas and Weiss, Pierre},
  journal={SIAM Journal on Imaging Sciences},
  volume={7},
  number={4},
  pages={1962--1992},
  year={2014},
  publisher={SIAM}
}

@article{wang2009variable,
  title={Variable density compressed image sampling},
  author={Wang, Zhongmin and Arce, Gonzalo R},
  journal={IEEE Transactions on image processing},
  volume={19},
  number={1},
  pages={264--270},
  year={2009},
  publisher={IEEE}
}

@article{adler2018learned,
  title={Learned primal-dual reconstruction},
  author={Adler, Jonas and {\"O}ktem, Ozan},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={6},
  pages={1322--1332},
  year={2018},
  publisher={IEEE}
}

@article{hammernik2018learning,
  title={Learning a variational network for reconstruction of accelerated MRI data},
  author={Hammernik, Kerstin and Klatzer, Teresa and Kobler, Erich and Recht, Michael P and Sodickson, Daniel K and Pock, Thomas and Knoll, Florian},
  journal={Magnetic resonance in medicine},
  volume={79},
  number={6},
  pages={3055--3071},
  year={2018},
  publisher={Wiley Online Library}
}

@inproceedings{mardani2018neural,
  title={Neural proximal gradient descent for compressive imaging},
  author={Mardani, Morteza and Sun, Qingyun and Donoho, David and Papyan, Vardan and Monajemi, Hatef and Vasanawala, Shreyas and Pauly, John},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9573--9583},
  year={2018}
}

@article{qin2018convolutional,
  title={Convolutional recurrent neural networks for dynamic MR image reconstruction},
  author={Qin, Chen and Schlemper, Jo and Caballero, Jose and Price, Anthony N and Hajnal, Joseph V and Rueckert, Daniel},
  journal={IEEE transactions on medical imaging},
  volume={38},
  number={1},
  pages={280--290},
  year={2018},
  publisher={IEEE}
}

@article{sherry2019learning,
  title={Learning the Sampling Pattern for {MRI}},
  author={Sherry, Ferdia and Benning, Martin and Reyes, Juan Carlos De los and Graves, Martin J and Maierhofer, Georg and Williams, Guy and Sch{\"o}nlieb, Carola-Bibiane and Ehrhardt, Matthias J},
  journal={arXiv preprint arXiv:1906.08754},
  year={2019}
}

@article{adler2018deep,
  title={Deep bayesian inversion},
  author={Adler, Jonas and {\"O}ktem, Ozan},
  journal={arXiv preprint arXiv:1811.05910},
  year={2018}
}

@article{weiss2019learning,
  title={Learning Fast Magnetic Resonance Imaging},
  author={Weiss, Tomer and Vedula, Sanketh and Senouf, Ortal and Bronstein, Alex and Michailovich, Oleg and Zibulevsky, Michael},
  journal={arXiv preprint arXiv:1905.09324},
  year={2019}
}

@article{jin2019self,
  title={Self-Supervised Deep Active Accelerated {MRI}},
  author={Jin, Kyong Hwan and Unser, Michael and Yi, Kwang Moo},
  journal={arXiv preprint arXiv:1901.04547},
  year={2019}
}

@inproceedings{zhang2019reducing,
  title={Reducing Uncertainty in Undersampled {MRI} Reconstruction with Active Acquisition},
  author={Zhang, Zizhao and Romero, Adriana and Muckley, Matthew J and Vincent, Pascal and Yang, Lin and Drozdzal, Michal},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2049--2058},
  year={2019}
}

@article{boyer2016generation,
  title={On the generation of sampling schemes for magnetic resonance imaging},
  author={Boyer, Claire and Chauffert, Nicolas and Ciuciu, Philippe and Kahn, Jonas and Weiss, Pierre},
  journal={SIAM Journal on Imaging Sciences},
  volume={9},
  number={4},
  pages={2039--2072},
  year={2016},
  publisher={SIAM}
}

@inproceedings{lazarus2017sparkling,
  title={{SPARKLING}: Novel Non-Cartesian Sampling Schemes for Accelerated {2D} Anatomical Imaging at {7T} Using Compressed Sensing},
  author={Lazarus, Carole and Weiss, Pierre and Chauffert, Nicolas and Mauconduit, Franck and Bottlaender, Michel and Vignaud, Alexandre and Ciuciu, Philippe},
  booktitle={25th annual meeting of the International Society for Magnetic Resonance Imaging},
  year={2017}
}

@article{becker2011nesta,
  title={NESTA: A fast and accurate first-order method for sparse recovery},
  author={Becker, Stephen and Bobin, J{\'e}r{\^o}me and Cand{\`e}s, Emmanuel J},
  journal={SIAM Journal on Imaging Sciences},
  volume={4},
  number={1},
  pages={1--39},
  year={2011},
  publisher={SIAM}
}


@inproceedings{gozcu2019rethinking,
  title={Rethinking Sampling in Parallel {MRI}: A Data-Driven Approach},
  author={G{\"o}zc{\"u}, Baran and Sanchez, Thomas and Cevher, Volkan},
  booktitle={27th European Signal Processing Conference (EUSIPCO)},
  year={2019}
}

@article{haldar2019oedipus,
  title={OEDIPUS: An experiment design framework for sparsity-constrained MRI},
  author={Haldar, Justin P and Kim, Daeun},
  journal={IEEE transactions on medical imaging},
  year={2019},
  publisher={IEEE}
}

@inproceedings{sanchez2019scalable,
  author={Sanchez, Thomas and G{\"o}zc{\"u}, Baran and van Heeswijk, Ruud B. and Eftekhari, Armin and Il{\i}cak, Efe and {\c{C}}ukur, Tolga and Cevher, Volkan},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Scalable Learning-Based Sampling Optimization for Compressive Dynamic {MRI}}, 
  year={2020},
  volume={},
  number={},
  pages={8584-8588},
  doi={10.1109/ICASSP40776.2020.9053345},
}


@inproceedings{vasanawala2011practical,
  title={Practical parallel imaging compressed sensing {MRI}: {Summary} of two years of experience in accelerating body {MRI} of pediatric patients},
  author={Vasanawala, SS and Murphy, MJ and Alley, Marcus T and Lai, P and Keutzer, Kurt and Pauly, John M and Lustig, Michael},
  booktitle={Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on},
  pages={1039--1043},
  year={2011},
  organization={IEEE}
}

@article{deshmane2012parallel,
  title={Parallel {MR} imaging},
  author={Deshmane, Anagha and Gulani, Vikas and Griswold, Mark A and Seiberlich, Nicole},
  journal={Journal of Magnetic Resonance Imaging},
  volume={36},
  number={1},
  pages={55--72},
  year={2012},
  publisher={Wiley Online Library}
}

@inproceedings{liu2012under,
  title={Under-sampling trajectory design for compressed sensing {MRI}},
  author={Liu, Duan-duan and Liang, Dong and Liu, Xin and Zhang, Yuan-ting},
  booktitle={Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE},
  pages={73--76},
  year={2012},
  organization={IEEE}
}

@article{zhang2014energy,
  title={Energy preserved sampling for compressed sensing {MRI}},
  author={Zhang, Yudong and Peterson, Bradley S and Ji, Genlin and Dong, Zhengchao},
  journal={Computational and mathematical methods in medicine},
  volume={2014},
  year={2014},
  publisher={Hindawi}
}

@article{knoll2011adapted,
  title={Adapted random sampling patterns for accelerated {MRI}},
  author={Knoll, Florian and Clason, Christian and Diwoky, Clemens and Stollberger, Rudolf},
  journal={Magnetic resonance materials in physics, biology and medicine},
  volume={24},
  number={1},
  pages={43--50},
  year={2011},
  publisher={Springer}
}

@article{candes2006robust,
  title={Robust uncertainty principles: {Exact} signal reconstruction from highly incomplete frequency information},
  author={Cand{\`e}s, Emmanuel J and Romberg, Justin and Tao, Terence},
  journal={IEEE Trans, on Inf. Theory},
  volume={52},
  number={2},
  pages={489--509},
  year={2006},
  publisher={IEEE}
}

@article{maggioni2012video,
  title={Video denoising, deblocking, and enhancement through separable {4-D} nonlocal spatiotemporal transforms},
  author={Maggioni, Matteo and Boracchi, Giacomo and Foi, Alessandro and Egiazarian, Karen},
  journal={IEEE Transactions on image processing},
  volume={21},
  number={9},
  pages={3952--3966},
  year={2012},
  publisher={IEEE}
}

@article{saeed2015cardiac,
  title={Cardiac {MR} imaging: current status and future direction},
  author={Saeed, Maythem and Van, Tu Anh and Krug, Roland and Hetts, Steven W and Wilson, Mark W},
  journal={Cardiovascular diagnosis and therapy},
  volume={5},
  number={4},
  pages={290},
  year={2015},
  publisher={AME Publications}
}

@misc{vistaonline,
title={Variable Density Incoherent Spatiotemporal Acquisition ({VISTA}) code},
author={Ahmad, Rizwan},
year={2013},
note={Available at \url{https://github.com/osu-cmr/vista} - Downloaded on the 17-05-2018}
}

@article{boyd2011distributed,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
  journal={Foundations and Trends{\textregistered} in Machine learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers, Inc.}
}

@article{wen2012solving,
  title={Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm},
  author={Wen, Zaiwen and Yin, Wotao and Zhang, Yin},
  journal={Mathematical Programming Computation},
  volume={4},
  number={4},
  pages={333--361},
  year={2012},
  publisher={Springer}
}

%% VARIABLE- DENSITY TECHNIQUES

@article{ahmad2015variable,
  title={Variable density incoherent spatiotemporal acquisition ({VISTA}) for highly accelerated cardiac {MRI}},
  author={Ahmad, Rizwan and Xue, Hui and Giri, Shivraman and Ding, Yu and Craft, Jason and Simonetti, Orlando P},
  journal={Magnetic Resonance in Medicine},
  volume={74},
  number={5},
  pages={1266--1278},
  year={2015},
  publisher={Wiley Online Library}
}

@article{li2018dynamic,
  title={Dynamic magnetic resonance imaging method based on golden-ratio cartesian sampling and compressed sensing},
  author={Li, Shuo and Zhu, Yanchun and Xie, Yaoqin and Gao, Song},
  journal={PloS one},
  volume={13},
  number={1},
  pages={e0191569},
  year={2018},
  publisher={Public Library of Science}
}


%% OTHERS
@article{yoon2014motion,
  title={Motion adaptive patch-based low-rank approach for compressed sensing cardiac cine {MRI}},
  author={Yoon, Huisu and Kim, Kyung Sang and Kim, Daniel and Bresler, Yoram and Ye, Jong Chul},
  journal={IEEE Transactions on Medical Imaging},
  volume={33},
  number={11},
  pages={2069--2085},
  year={2014},
  publisher={IEEE}
}

@article{schlemper2018deep,
  title={A deep cascade of convolutional neural networks for dynamic {MR} image reconstruction},
  author={Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V and Price, Anthony N and Rueckert, Daniel},
  journal={IEEE Transactions on Medical Imaging},
  volume={37},
  number={2},
  pages={491--503},
  year={2018},
  publisher={IEEE}
}

%% Greedy algorithm theory - Non-submodular optimisation

@article{bogunovic2018robust,
  title={Robust Maximization of Non-Submodular Objectives},
  author={Bogunovic, Ilija and Zhao, Junyao and Cevher, Volkan},
  journal={arXiv preprint arXiv:1802.07073},
  year={2018}
}

@article{elenberg2016restricted,
  title={Restricted strong convexity implies weak submodularity},
  author={Elenberg, Ethan R and Khanna, Rajiv and Dimakis, Alexandros G and Negahban, Sahand},
  journal={arXiv preprint arXiv:1612.00804},
  year={2016}
}

@article{khanna2017scalable,
  title={Scalable greedy feature selection via weak submodularity},
  author={Khanna, Rajiv and Elenberg, Ethan and Dimakis, Alexandros G and Negahban, Sahand and Ghosh, Joydeep},
  journal={arXiv preprint arXiv:1703.02723},
  year={2017}
}



@inproceedings{buchbinder2014submodular,
  title={Submodular maximization with cardinality constraints},
  author={Buchbinder, Niv and Feldman, Moran and Naor, Joseph Seffi and Schwartz, Roy},
  booktitle={Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms},
  pages={1433--1452},
  year={2014},
  organization={Society for Industrial and Applied Mathematics}
}

@article{bahmani2013greedy,
  title={Greedy sparsity-constrained optimization},
  author={Bahmani, Sohail and Raj, Bhiksha and Boufounos, Petros T},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Mar},
  pages={807--841},
  year={2013}
}

@inproceedings{loh2013regularized,
  title={Regularized {M}-estimators with nonconvexity: Statistical and algorithmic theory for local optima},
  author={Loh, Po-Ling and Wainwright, Martin J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={476--484},
  year={2013}
}

@article{das2011submodular,
  title={Submodular meets spectral: {Greedy} algorithms for subset selection, sparse approximation and dictionary selection},
  author={Das, Abhimanyu and Kempe, David},
  journal={arXiv preprint arXiv:1102.3975},
  year={2011}
}


@inproceedings{krause2010submodular,
  title={Submodular dictionary selection for sparse representation},
  author={Krause, Andreas and Cevher, Volkan},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={567--574},
  year={2010}
}


@article{jin2016general,
  title={A general framework for compressed sensing and parallel {MRI} using annihilating filter based low-rank {Hankel} matrix},
  author={Jin, Kyong Hwan and Lee, Dongwook and Ye, Jong Chul},
  journal={IEEE Transactions on Computational Imaging},
  volume={2},
  number={4},
  pages={480--495},
  year={2016},
  publisher={IEEE}
}

@article{roman2014asymptotic,
  title={On asymptotic structure in compressed sensing},
  author={Roman, Bogdan and Hansen, Anders and Adcock, Ben},
  journal={arXiv preprint arXiv:1406.4178},
  year={2014}
}


@article{vellagoundar2015robust,
  title={A robust adaptive sampling method for faster acquisition of {MR} images},
  author={Vellagoundar, Jaganathan and Machireddy, Ramasubba Reddy},
  journal={Magnetic resonance imaging},
  volume={33},
  number={5},
  pages={635--643},
  year={2015},
  publisher={Elsevier}
}


@inproceedings{ponomarenko2007between,
  title={On between-coefficient contrast masking of {DCT} basis functions},
  author={Ponomarenko, Nikolay and Silvestri, Flavia and Egiazarian, Karen and Carli, Marco and Astola, Jaakko and Lukin, Vladimir},
  booktitle={Proceedings of the third international workshop on video processing and quality metrics},
  volume={4},
  year={2007}
}




@article{gorodnitsky1997sparse,
  title={Sparse signal reconstruction from limited data using {FOCUSS}: A re-weighted minimum norm algorithm},
  author={Gorodnitsky, Irina F and Rao, Bhaskar D},
  journal={IEEE Transactions on signal processing},
  volume={45},
  number={3},
  pages={600--616},
  year={1997},
  publisher={IEEE}
}


@article{santos2006single,
  title={Single breath-hold whole-heart {MRA} using variable-density spirals at 3t},
  author={Santos, Juan M and Cunningham, Charles H and Lustig, Michael and Hargreaves, Brian A and Hu, Bob S and Nishimura, Dwight G and Pauly, John M},
  journal={Magnetic Resonance in Medicine},
  volume={55},
  number={2},
  pages={371--379},
  year={2006},
  publisher={Wiley Online Library}
}





@article{frahm1986rapid,
  title={Rapid {NMR} imaging of dynamic processes using the {FLASH} technique},
  author={Frahm, Jens and Haase, Axel and Matthaei, Dieter},
  journal={Magnetic Resonance in Medicine},
  volume={3},
  number={2},
  pages={321--327},
  year={1986},
  publisher={Wiley Online Library}
}


@article{zhang2010real,
  title={Real-time cardiovascular magnetic resonance at high temporal resolution: radial {FLASH} with nonlinear inverse reconstruction},
  author={Zhang, Shuo and Uecker, Martin and Voit, Dirk and Merboldt, Klaus-Dietmar and Frahm, Jens},
  journal={Journal of Cardiovascular Magnetic Resonance},
  volume={12},
  number={1},
  pages={39},
  year={2010},
  publisher={BioMed Central}
}

@article{kellman2001adaptive,
  title={Adaptive sensitivity encoding incorporating temporal filtering ({TSENSE})},
  author={Kellman, Peter and Epstein, Frederick H and McVeigh, Elliot R},
  journal={Magnetic Resonance in Medicine},
  volume={45},
  number={5},
  pages={846--852},
  year={2001},
  publisher={Wiley Online Library}
}

@article{axel2016accelerated,
  title={Accelerated {MRI} for the assessment of cardiac function},
  author={Axel, Leon and Otazo, Ricardo},
  journal={The British journal of radiology},
  volume={89},
  number={1063},
  pages={20150655},
  year={2016},
  publisher={The British Institute of Radiology.}
}





@article{fessler2001iterative,
  title={Iterative tomographic image reconstruction using nonuniform fast {Fourier} transforms},
  author={Fessler, Jeffrey A},
  journal={Commun. Signal Process. Lab., Dept. Elect. Eng. Comput. Sci., Univ. Michigan, Ann Arbor, MI, Tech. Rep},
  year={2001}
}


@article{rasche1999resampling,
  title={Resampling of data between arbitrary grids using convolution interpolation},
  author={Rasche, Volker and Proksa, Roland and Sinkus, R and Bornert, Peter and Eggers, Holger},
  journal={IEEE Transactions on Medical Imaging},
  volume={18},
  number={5},
  pages={385--392},
  year={1999},
  publisher={IEEE}
}


@article{pauly2007non,
  title={Non-cartesian reconstruction},
  author={Pauly, John},
  journal={URL: http://www. stanford. edu/class/ee369c/notes/non cart rec},
  volume={7},
  year={2007}
}

@article{weizman2015compressed,
  title={Compressed sensing for longitudinal {MRI}: An adaptive-weighted approach},
  author={Weizman, Lior and Eldar, Yonina C and Ben Bashat, Dafna},
  journal={Medical physics},
  volume={42},
  number={9},
  pages={5195--5208},
  year={2015},
  publisher={Wiley Online Library}
}


@article{wang2014compressed,
  title={Compressed sensing dynamic cardiac cine {MRI} using learned spatiotemporal dictionary},
  author={Wang, Yanhua and Ying, Leslie},
  journal={IEEE transactions on Biomedical Engineering},
  volume={61},
  number={4},
  pages={1109--1120},
  year={2014},
  publisher={IEEE}
}


@article{otazo2015low,
  title={Low-rank plus sparse matrix decomposition for accelerated dynamic {MRI} with separation of background and dynamic components},
  author={Otazo, Ricardo and Cand{\`e}s, Emmanuel and Sodickson, Daniel K},
  journal={Magnetic Resonance in Medicine},
  volume={73},
  number={3},
  pages={1125--1136},
  year={2015},
  publisher={Wiley Online Library}
}


@article{uecker2010real,
  title={Real-time {MRI} at a resolution of 20 ms},
  author={Uecker, Martin and Zhang, Shuo and Voit, Dirk and Karaus, Alexander and Merboldt, Klaus-Dietmar and Frahm, Jens},
  journal={NMR in Biomedicine},
  volume={23},
  number={8},
  pages={986--994},
  year={2010},
  publisher={Wiley Online Library}
}


@article{glockner2005parallel,
  title={Parallel {MR} imaging: a user's guide},
  author={Glockner, James F and Hu, Houchun H and Stanley, David W and Angelos, Lisa and King, Kevin},
  journal={Radiographics},
  volume={25},
  number={5},
  pages={1279--1297},
  year={2005},
  publisher={Radiological Society of North America}
}


@article{roemer1990nmr,
  title={The {NMR} phased array},
  author={Roemer, Peter B and Edelstein, William A and Hayes, Cecil E and Souza, Steven P and Mueller, OM},
  journal={Magnetic Resonance in Medicine},
  volume={16},
  number={2},
  pages={192--225},
  year={1990},
  publisher={Wiley Online Library}
}


@article{sodickson1997simultaneous,
  title={Simultaneous acquisition of spatial harmonics (SMASH): fast imaging with radiofrequency coil arrays},
  author={Sodickson, Daniel K and Manning, Warren J},
  journal={Magnetic Resonance in Medicine},
  volume={38},
  number={4},
  pages={591--603},
  year={1997},
  publisher={Wiley Online Library}
}


@article{blaimer2004smash,
  title={{SMASH}, {SENSE}, {PILS}, {GRAPPA}: how to choose the optimal method},
  author={Blaimer, Martin and Breuer, Felix and Mueller, Matthias and Heidemann, Robin M and Griswold, Mark A and Jakob, Peter M},
  journal={Topics in Magnetic Resonance Imaging},
  volume={15},
  number={4},
  pages={223--236},
  year={2004},
  publisher={LWW}
}


@article{pruessmann1999sense,
  title={{SENSE}: sensitivity encoding for fast {MRI}},
  author={Pruessmann, Klaas P and Weiger, Markus and Scheidegger, Markus B and Boesiger, Peter and others},
  journal={Magnetic Resonance in Medicine},
  volume={42},
  number={5},
  pages={952--962},
  year={1999}
}




@article{kim2012accelerated,
  title={Accelerated phase-contrast cine {MRI} using k-t {SPARSE-SENSE}},
  author={Kim, Daniel and Dyvorne, Hadrien A and Otazo, Ricardo and Feng, Li and Sodickson, Daniel K and Lee, Vivian S},
  journal={Magnetic Resonance in Medicine},
  volume={67},
  number={4},
  pages={1054--1064},
  year={2012},
  publisher={Wiley Online Library}
}



@article{tsao2003k,
  title={k-t {BLAST} and k-t {SENSE}: Dynamic {MRI} with high frame rate exploiting spatiotemporal correlations},
  author={Tsao, Jeffrey and Boesiger, Peter and Pruessmann, Klaas P},
  journal={Magnetic Resonance in Medicine},
  volume={50},
  number={5},
  pages={1031--1042},
  year={2003},
  publisher={Wiley Online Library}
}


@inproceedings{griswold2002use,
  title={The use of an adaptive reconstruction for array coil sensitivity mapping and intensity normalization},
  author={Griswold, Mark A and Walsh, David and Heidemann, Robin M and Haase, Axel and Jakob, Peter M},
  booktitle={International Society for Magnetic Resonance in Medicine (ISMRM), Proceedings of the 10th Scientific Meeting},
  pages={2410},
  year={2002}
}

@article{walsh2000adaptive,
  title={Adaptive reconstruction of phased array MR imagery},
  author={Walsh, David O and Gmitro, Arthur F and Marcellin, Michael W},
  journal={Magnetic Resonance in Medicine},
  volume={43},
  number={5},
  pages={682--690},
  year={2000},
  publisher={Wiley Online Library}
}


@article{feng2013highly,
  title={Highly accelerated real-time cardiac cine {MRI} using $k-t$ {SPARSE-SENSE}},
  author={Feng, Li and Srichai, Monvadi B and Lim, Ruth P and Harrison, Alexis and King, Wilson and Adluru, Ganesh and Dibella, Edward VR and Sodickson, Daniel K and Otazo, Ricardo and Kim, Daniel},
  journal={Magnetic Resonance in Medicine},
  volume={70},
  number={1},
  pages={64--74},
  year={2013},
  publisher={Wiley Online Library}
}


@misc{blink2004mri,
  title={{MRI}: Physics},
  author={Blink, Evert J},
  year={2004},
  note = {\textit{Available at \url{http://www.mri-physics.net/bin/mri-physics-en-rev1.3.pdf} - Consulted on the 9-11-2017}}
}

@book{chavhan2013mri,
  title={{MRI} made easy},
  author={Chavhan, Govind B},
  year={2013},
  publisher={JP Medical Ltd}
}



@book{nessaiver1996all,
  title={All you really need to know about {MRI} physics},
  author={NessAiver, Moriel},
  year={1996},
  publisher={University of Maryland, Medical Center}
}

@misc{qmri,
author = {Elster, Allen D.},
title  = {Questions and Answers in {MRI}},
note    = {\textit{Available at \url{http://mriquestions.com} - Consulted on the 24-10-2017}},
year={2021},
}

@article{raja2014adaptive,
  title={Adaptive k-space sampling design for edge-enhanced {DCE-MRI} using compressed sensing},
  author={Raja, Rajikha and Sinha, Neelam},
  journal={Magnetic resonance imaging},
  volume={32},
  number={7},
  pages={899--912},
  year={2014},
  publisher={Elsevier}
}

@article{gozcu2018learning,
author={G\"ozc\"u, Baran and Mahabadi, Rabeeh K. and Li, Yen-Huan and Il{\i}cak, Efe and \c{C}ukur, Tolga and Scarlett, Jonathan and Cevher, Volkan},
title={Learning-based compressive {MRI}},
journal={ IEEE Transactions on Medical Imaging},
year={2018}
}




@article{chen2016real,
  title={Real time dynamic {MRI} by exploiting spatial and temporal sparsity},
  author={Chen, Chen and Li, Yeqing and Axel, Leon and Huang, Junzhou},
  journal={Magnetic resonance imaging},
  volume={34},
  number={4},
  pages={473--482},
  year={2016},
  publisher={Elsevier}
}

@article{zhang2010magnetic,
  title={Magnetic resonance imaging in real time: advances using radial FLASH},
  author={Zhang, Shuo and Block, Kai Tobias and Frahm, Jens},
  journal={Journal of Magnetic Resonance Imaging},
  volume={31},
  number={1},
  pages={101--109},
  year={2010},
  publisher={Wiley Online Library}
}


@article{lingala2011accelerated,
  title={Accelerated dynamic {MRI} exploiting sparsity and low-rank structure: k-t {SLR}},
  author={Lingala, Sajan Goud and Hu, Yue and DiBella, Edward and Jacob, Mathews},
  journal={IEEE Transactions on Medical Imaging},
  volume={30},
  number={5},
  pages={1042--1054},
  year={2011},
  publisher={IEEE}
}

@inproceedings{yao2015accelerated,
  title={Accelerated dynamic {MRI} reconstruction with total variation and nuclear norm regularization},
  author={Yao, Jiawen and Xu, Zheng and Huang, Xiaolei and Huang, Junzhou},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={635--642},
  year={2015},
  organization={Springer}
}


@inproceedings{chen2014real,
  title={Real time dynamic {MRI} with dynamic total variation},
  author={Chen, Chen and Li, Yeqing and Axel, Leon and Huang, Junzhou},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={138--145},
  year={2014},
  organization={Springer}
}

@article{knoll2011second,
  title={Second order total generalized variation (TGV) for MRI},
  author={Knoll, Florian and Bredies, Kristian and Pock, Thomas and Stollberger, Rudolf},
  journal={Magnetic resonance in medicine},
  volume={65},
  number={2},
  pages={480--491},
  year={2011},
  publisher={Wiley Online Library}
}



@inproceedings{lustig2006kt,
  title={$k-t$ {SPARSE}: High frame rate dynamic {MRI} exploiting spatio-temporal sparsity},
  author={Lustig, Michael and Santos, Juan M and Donoho, David L and Pauly, John M},
  booktitle={Proc. of the 13th Annual Meeting of ISMRM, Seattle},
  volume={2420},
  year={2006}
}


@article{o1985fast,
  title={A fast sinc function gridding algorithm for Fourier inversion in computer tomography},
  author={O'sullivan, JD},
  journal={IEEE Transactions on Medical Imaging},
  volume={4},
  number={4},
  pages={200--207},
  year={1985},
  publisher={IEEE}
}

@article{jackson1991selection,
  title={Selection of a convolution function for Fourier inversion using gridding (computerised tomography application)},
  author={Jackson, John I and Meyer, Craig H and Nishimura, Dwight G and Macovski, Albert},
  journal={IEEE Transactions on Medical Imaging},
  volume={10},
  number={3},
  pages={473--478},
  year={1991},
  publisher={IEEE}
}

@article{fessler2003nonuniform,
  title={Nonuniform fast Fourier transforms using min-max interpolation},
  author={Fessler, Jeffrey A and Sutton, Bradley P},
  journal={IEEE Transactions on Signal Processing},
  volume={51},
  number={2},
  pages={560--574},
  year={2003},
  publisher={IEEE}
}

@article{donoho2006compressed,
  title={Compressed sensing},
  author={Donoho, David L},
  journal={IEEE transactions on Information Theory},
  volume={52},
  number={4},
  pages={1289--1306},
  year={2006},
  publisher={IEEE}
}

@article{jung2007improved,
  title={Improved k--t {BLAST} and k--t {SENSE} using {FOCUSS}},
  author={Jung, Hong and Ye, Jong Chul and Kim, Eung Yeop},
  journal={Physics in medicine and biology},
  volume={52},
  number={11},
  pages={3201},
  year={2007},
  publisher={IOP Publishing}
}

@article{fessler2007nufft,
  title={On {NUFFT}-based gridding for non-Cartesian {MRI}},
  author={Fessler, Jeffrey A},
  journal={Journal of Magnetic Resonance},
  volume={188},
  number={2},
  pages={191--195},
  year={2007},
  publisher={Elsevier}
}


@article{lustig2008compressed,
  title={Compressed sensing {MRI}},
  author={Lustig, Michael and Donoho, David L and Santos, Juan M and Pauly, John M},
  journal={IEEE signal processing magazine},
  volume={25},
  number={2},
  pages={72--82},
  year={2008},
  publisher={IEEE}
}



@article{jung2009k,
  title={k-t {FOCUSS}: A general compressed sensing framework for high resolution dynamic {MRI}},
  author={Jung, Hong and Sung, Kyunghyun and Nayak, Krishna S and Kim, Eung Yeop and Ye, Jong Chul},
  journal={Magn. Reson. Med.},
  volume={61},
  number={1},
  pages={103--116},
  year={2009},
  publisher={Wiley Online Library}
}
  %journal={Magnetic Resonance in Medicine},


@article{seeger2010optimization,
  title={Optimization of k-space trajectories for compressed sensing by Bayesian experimental design},
  author={Seeger, Matthias and Nickisch, Hannes and Pohmann, Rolf and Sch{\"o}lkopf, Bernhard},
  journal={Magn. Reson. Med.},
  volume={63},
  number={1},
  pages={116--126},
  year={2010},
  publisher={Wiley Online Library}
}

@article{ravishankar2011mr,
  title={{MR} image reconstruction from highly undersampled k-space data by dictionary learning},
  author={Ravishankar, Saiprasad and Bresler, Yoram},
  journal={IEEE Transactions on Medical Imaging},
  volume={30},
  number={5},
  pages={1028--1041},
  year={2011},
  publisher={IEEE}
}

@article{tsao2012mri,
  title={{MRI} temporal acceleration techniques},
  author={Tsao, Jeffrey and Kozerke, Sebastian},
  journal={Journal of Magnetic Resonance Imaging},
  volume={36},
  number={3},
  pages={543--560},
  year={2012},
  publisher={Wiley Online Library}
}



@article{feng2014golden,
  title={Golden-angle radial sparse parallel {MRI}: Combination of compressed sensing, parallel imaging, and golden-angle radial sampling for fast and flexible dynamic volumetric  {MRI}},
  author={Feng, Li and Grimm, Robert and Block, Kai Tobias and Chandarana, Hersh and Kim, Sungheon and Xu, Jian and Axel, Leon and Sodickson, Daniel K and Otazo, Ricardo},
  journal={Magnetic Resonance in Medicine},
  volume={72},
  number={3},
  pages={707--717},
  year={2014},
  publisher={Wiley Online Library}
}

@article{baldassarre2016learning,
  title={Learning-based compressive subsampling},
  author={Baldassarre, Luca and Li, Yen-Huan and Scarlett, Jonathan and G{\"o}zc{\"u}, Baran and Bogunovic, Ilija and Cevher, Volkan},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={10},
  number={4},
  pages={809--822},
  year={2016},
  publisher={IEEE}
}

@inproceedings{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Advances in neural information processing systems},
  pages={6626--6637},
  year={2017}
}

@article{mescheder2018training,
  title={Which training methods for GANs do actually converge?},
  author={Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
  journal={arXiv preprint arXiv:1801.04406},
  year={2018}
}
@inproceedings{
petzka2018on,
title={On the regularization of Wasserstein {GAN}s},
author={Henning Petzka and Asja Fischer and Denis Lukovnikov},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1hYRMbCW},
}

%% Saved with string encoding Unicode (UTF-8) 


@article{kreutz2003dictionary,
  title={Dictionary learning algorithms for sparse representation},
  author={Kreutz-Delgado, Kenneth and Murray, Joseph F and Rao, Bhaskar D and Engan, Kjersti and Lee, Te-Won and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={15},
  number={2},
  pages={349--396},
  year={2003},
  publisher={MIT Press}
}


@inproceedings{otazo2012combination,
  title={Combination of compressed sensing and parallel imaging for highly-accelerated dynamic {MRI}},
  author={Otazo, Ricardo and Feng, Li and Chandarana, Hersh and Block, Tobias and Axel, Leon and Sodickson, Daniel K},
  booktitle={Biomedical Imaging (ISBI), 2012 9th IEEE International Symposium on},
  pages={980--983},
  year={2012},
  organization={IEEE}
}

@article{otazo2010combination,
  title={Combination of compressed sensing and parallel imaging for highly accelerated first-pass cardiac perfusion {MRI}},
  author={Otazo, Ricardo and Kim, Daniel and Axel, Leon and Sodickson, Daniel K},
  journal={Magnetic Resonance in Medicine},
  volume={64},
  number={3},
  pages={767--776},
  year={2010},
  publisher={Wiley Online Library}
}




@misc{rosenmr,
  title={{MR} Image Encoding},
  author={Rosen, Bruce and Wald, Lawrence},
  year={2006},
  note={\textit{Available at \url{http://mriquestions.com/uploads/3/4/5/7/34572113/imageencoding_mit_courseware_wald.pdf} (Consulted on the 10-11-2017)}}
}



@article{vaswani2010modified,
  title={Modified-{CS}: Modifying compressive sensing for problems with partially known support},
  author={Vaswani, Namrata and Lu, Wei},
  journal={IEEE Transactions on Signal Processing},
  volume={58},
  number={9},
  pages={4595--4607},
  year={2010},
  publisher={IEEE}
}


@article{tremoulheac2014dynamic,
  title={Dynamic {MR} Image Reconstruction--Separation From Undersampled (k,t)-Space via Low-Rank Plus Sparse Prior},
  author={Tr{\'e}moulh{\'e}ac, Benjamin and Dikaios, Nikolaos and Atkinson, David and Arridge, Simon R},
  journal={IEEE transactions on medical imaging},
  volume={33},
  number={8},
  pages={1689--1701},
  year={2014},
  publisher={IEEE}
}


@article{adluru2009acquisition,
  title={Acquisition and reconstruction of undersampled radial data for myocardial perfusion magnetic resonance imaging},
  author={Adluru, Ganesh and McGann, Chris and Speier, Peter and Kholmovski, Eugene G and Shaaban, Akram and DiBella, Edward VR},
  journal={Journal of Magnetic Resonance Imaging},
  volume={29},
  number={2},
  pages={466--473},
  year={2009},
  publisher={Wiley Online Library}
}



@article{candes2006stable,
  title={Stable signal recovery from incomplete and inaccurate measurements},
  author={Candes, Emmanuel J and Romberg, Justin K and Tao, Terence},
  journal={Communications on pure and applied mathematics},
  volume={59},
  number={8},
  pages={1207--1223},
  year={2006},
  publisher={Wiley Online Library}
}


@article{feng2016compressed,
  title={Compressed sensing for body {MRI}},
  author={Feng, Li and Benkert, Thomas and Block, Kai Tobias and Sodickson, Daniel K and Otazo, Ricardo and Chandarana, Hersh},
  journal={Journal of Magnetic Resonance Imaging},
  year={2016},
  publisher={Wiley Online Library}
}

@article{jung2010radial,
  title={Radial k-t {FOCUSS} for high-resolution cardiac cine {MRI}},
  author={Jung, Hong and Park, Jaeseok and Yoo, Jaeheung and Ye, Jong Chul},
  journal={Magnetic Resonance in Medicine},
  volume={63},
  number={1},
  pages={68--78},
  year={2010},
  publisher={Wiley Online Library}
}


@article{majumdar2012compressed,
  title={Compressed sensing based real-time dynamic {MRI} reconstruction},
  author={Majumdar, Angshul and Ward, Rabab K and Aboulnasr, Tyseer},
  journal={IEEE Transactions on Medical Imaging},
  volume={31},
  number={12},
  pages={2253--2266},
  year={2012},
  publisher={IEEE}
}


@article{zhao2012image,
  title={Image reconstruction from highly undersampled $(k,t)$-space data with joint partial separability and sparsity constraints},
  author={Zhao, Bo and Haldar, Justin P and Christodoulou, Anthony G and Liang, Zhi-Pei},
  journal={IEEE transactions on medical imaging},
  volume={31},
  number={9},
  pages={1809--1820},
  year={2012},
  publisher={IEEE}
}



@article{feng2016xd,
  title={XD-GRASP: Golden-angle radial {MRI} with reconstruction of extra motion-state dimensions using compressed sensing},
  author={Feng, Li and Axel, Leon and Chandarana, Hersh and Block, Kai Tobias and Sodickson, Daniel K and Otazo, Ricardo},
  journal={Magnetic resonance in medicine},
  volume={75},
  number={2},
  pages={775--788},
  year={2016},
  publisher={Wiley Online Library}
}


@techreport{atc13,
	Address = {California},
	Author = {{Applied Technology Council}},
	Date-Modified = {2011-03-21 13:46:44 +0100},
	Institution = {Seismic Safety Commission, Applied Technology Council (\textsc{atc})},
	Keywords = {ATC},
	Owner = {oropeza},
	Timestamp = {2008.01.11},
	Title = {{E}arthquake damage evaluation data for {C}alifornia},
	Year = {1985}}


@inproceedings{sriram2020end,
  title={End-to-end variational networks for accelerated MRI reconstruction},
  author={Sriram, Anuroop and Zbontar, Jure and Murrell, Tullie and Defazio, Aaron and Zitnick, C Lawrence and Yakubova, Nafissa and Knoll, Florian and Johnson, Patricia},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={64--73},
  year={2020},
  organization={Springer}
}

@inproceedings{
zhangFixupInitializationResidual2018,
title={Fixup {{Initialization}}: Residual Learning Without Normalization},
author={Hongyi Zhang and Yann N. Dauphin and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1gsz30cKX},
}



@article{zibetti2020fast,
  title={Fast Data-Driven Learning of MRI Sampling Pattern for Large Scale Problems},
  author={Zibetti, Marcelo VW and Herman, Gabor T and Regatte, Ravinder R},
  journal={arXiv preprint arXiv:2011.02322},
  year={2020}
}

@article{yang2017admm,
  title={ADMM-Net: A deep learning approach for compressive sensing MRI},
  author={Yang, Yan and Sun, Jian and Li, Huibin and Xu, Zongben},
  journal={arXiv preprint arXiv:1705.06869},
  year={2017}
}

@inproceedings{aggarwal2020joint,
  title={Joint optimization of sampling patterns and deep priors for improved parallel MRI},
  author={Aggarwal, Hemant K and Jacob, Mathews},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8901--8905},
  year={2020},
  organization={IEEE}
}


@article{aggarwal2020j,
  title={J-MoDL: Joint model-based deep learning for optimized sampling and reconstruction},
  author={Aggarwal, Hemant Kumar and Jacob, Mathews},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={14},
  number={6},
  pages={1151--1162},
  year={2020},
  publisher={IEEE}
}


@article{yin2021end,
  title={End-to-End Sequential Sampling and Reconstruction for MR Imaging},
  author={Yin, Tianwei and Wu, Zihui and Sun, He and Dalca, Adrian V and Yue, Yisong and Bouman, Katherine L},
  journal={arXiv preprint arXiv:2105.06460},
  year={2021}
}


@misc{
gorp2021active,
title={Active Deep Probabilistic Subsampling},
author={Hans van Gorp and Iris A.M. Huijben and Bastiaan S. Veeling and Nicola Pezzotti and Ruud Van Sloun},
year={2021},
url={https://openreview.net/forum?id=0NQdxInFWT_}
}



@inproceedings{weiss2020joint,
  title={Joint learning of Cartesian under sampling and reconstruction for accelerated MRI},
  author={Weiss, Tomer and Vedula, Sanketh and Senouf, Ortal and Michailovich, Oleg and Zibulevsky, Michael and Bronstein, Alex},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8653--8657},
  year={2020},
  organization={IEEE}
}

@inproceedings{narnhofer2019inverse,
  title={Inverse GANs for accelerated MRI reconstruction},
  author={Narnhofer, Dominik and Hammernik, Kerstin and Knoll, Florian and Pock, Thomas},
  booktitle={Wavelets and Sparsity XVIII},
  volume={11138},
  pages={111381A},
  year={2019},
  organization={International Society for Optics and Photonics}
}

@inproceedings{germain2015made,
  title={Made: Masked autoencoder for distribution estimation},
  author={Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle={International Conference on Machine Learning},
  pages={881--889},
  year={2015},
  organization={PMLR}
}


@article{vincent2010stacked,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  journal={Journal of machine learning research},
  volume={11},
  number={12},
  year={2010}
}


@inproceedings{yoon2018gain,
  title={Gain: Missing data imputation using generative adversarial nets},
  author={Yoon, Jinsung and Jordon, James and Schaar, Mihaela},
  booktitle={International Conference on Machine Learning},
  pages={5689--5698},
  year={2018},
  organization={PMLR}
}


@article{cannella2020projected,
  title={Projected Latent Markov Chain Monte Carlo: Conditional Inference with Normalizing Flows},
  author={Cannella, Chris and Soltani, Mohammadreza and Tarokh, Vahid},
  journal={arXiv preprint arXiv:2007.06140},
  year={2020}
}


@article{whang2020approximate,
  title={Approximate Probabilistic Inference with Composed Flows},
  author={Whang, Jay and Lindgren, Erik M and Dimakis, Alexandros G},
  journal={arXiv preprint arXiv:2002.11743},
  year={2020}
}

@InProceedings{li2019acflow, 
  title = {{ACF}low: Flow Models for Arbitrary Conditional Likelihoods}, 
  author = {Li, Yang and Akbar, Shoaib and Oliva, Junier}, 
  booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
  pages = {5831--5841}, 
  year = {2020}, 
  editor = {Hal Daumé III and Aarti Singh}, 
  volume = {119}, 
  series = {Proceedings of Machine Learning Research}, 
  month = {07}, 
  publisher = {PMLR}, 
  pdf = {http://proceedings.mlr.press/v119/li20a/li20a.pdf}, 
  url = { http://proceedings.mlr.press/v119/li20a.html} 
} 

@article{kovachki2020conditional,
  title={Conditional Sampling With Monotone GANs},
  author={Kovachki, Nikola and Baptista, Ricardo and Hosseini, Bamdad and Marzouk, Youssef},
  journal={arXiv preprint arXiv:2006.06755},
  year={2020}
}


@article{bakker2020experimental,
	title={Experimental design for MRI by greedy policy search},
	author={Bakker, Tim and van Hoof, Herke and Welling, Max},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	year={2020}
}
@inproceedings{
	sanchez2020uncertaintydriven,
	title={Uncertainty-Driven Adaptive Sampling via {GAN}s},
	author={Thomas Sanchez and Igor Krawczuk and Zhaodong Sun and Volkan Cevher},
	booktitle={NeurIPS 2020 Workshop on Deep Learning and Inverse Problems},
	year={2020},
	note={Available at \url{https://openreview.net/pdf?id=lWLYCQmtvW}}
}


@article{simard2003best,
  title={Best practices for convolutional neural networks applied to visual document analysis.},
  author={Simard, Patrice Y and Steinkraus, David and Platt, John C and others},
  journal={Icdar},
  volume={3},
  number={2003},
  year={2003}
}


@article{cheng2019compressed,
  title={Compressed Sensing: From Research to Clinical Practice with Data-Driven Learning},
  author={Cheng, Joseph Y and Chen, Feiyu and Sandino, Christopher and Mardani, Morteza and Pauly, John M and Vasanawala, Shreyas S},
  journal={arXiv preprint arXiv:1903.07824},
  year={2019}
}



@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

@article{yang2017dagan,
  title={DAGAN: deep de-aliasing generative adversarial networks for fast compressed sensing MRI reconstruction},
  author={Yang, Guang and Yu, Simiao and Dong, Hao and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Guo, Yike and others},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={6},
  pages={1310--1321},
  year={2017},
  publisher={IEEE}
}


@article{levi2019evaluating,
  title={Evaluating and Calibrating Uncertainty Prediction in Regression Tasks},
  author={Levi, Dan and Gispan, Liran and Giladi, Niv and Fetaya, Ethan},
  journal={arXiv preprint arXiv:1905.11659},
  year={2019}
}

@article{massart2007concentration,
  title={Concentration inequalities and model selection},
  author={Massart, Pascal},
  year={2007},
  publisher={Springer}
}

@inproceedings{wang2009pseudo,
  title={Pseudo 2D random sampling for compressed sensing MRI},
  author={Wang, Haifeng and Liang, Dong and Ying, Leslie},
  booktitle={2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  pages={2672--2675},
  year={2009},
  organization={IEEE}
}




@article{de2018clinically,
  title={Clinically applicable deep learning for diagnosis and referral in retinal disease},
  author={De Fauw, Jeffrey and Ledsam, Joseph R and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O?Donoghue, Brendan and Visentin, Daniel and others},
  journal={Nature medicine},
  volume={24},
  number={9},
  pages={1342},
  year={2018},
  publisher={Nature Publishing Group}
}


@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1321--1330},
  year={2017},
  organization={JMLR. org}
}



@article{yoon2012fast,
  title={Fast joint design method for parallel excitation radiofrequency pulse and gradient waveforms considering off-resonance},
  author={Yoon, Daehyun and Fessler, Jeffrey A and Gilbert, Anna C and Noll, Douglas C},
  journal={Magnetic resonance in medicine},
  volume={68},
  number={1},
  pages={278--285},
  year={2012},
  publisher={Wiley Online Library}
}

@article{block2007undersampled,
  title={Undersampled radial MRI with multiple coils. Iterative image reconstruction using a total variation constraint},
  author={Block, Kai Tobias and Uecker, Martin and Frahm, Jens},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={57},
  number={6},
  pages={1086--1098},
  year={2007},
  publisher={Wiley Online Library}
}


@article{bigot2016analysis,
  title={An analysis of block sampling strategies in compressed sensing},
  author={Bigot, J{\'e}r{\'e}mie and Boyer, Claire and Weiss, Pierre},
  journal={IEEE transactions on Information Theory},
  volume={62},
  number={4},
  pages={2125--2139},
  year={2016},
  publisher={IEEE}
}

@incollection{adcock2015quest,
  title={The quest for optimal sampling: {Computationally} efficient, structure-exploiting measurements for compressed sensing},
  author={Adcock, Ben and Hansen, Anders C and Roman, Bogdan},
  booktitle={Compressed Sensing and its Applications},
  pages={143--167},
  year={2015},
  publisher={Springer}
}



@article{wen2012solving,
  title={Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm},
  author={Wen, Zaiwen and Yin, Wotao and Zhang, Yin},
  journal={Mathematical Programming Computation},
  volume={4},
  number={4},
  pages={333--361},
  year={2012},
  publisher={Springer}
}


%% Greedy algorithm theory - Non-submodular optimisation

@article{bian2017guarantees,
  title={Guarantees for greedy maximization of non-submodular functions with applications},
  author={Bian, Andrew An and Buhmann, Joachim M and Krause, Andreas and Tschiatschek, Sebastian},
  journal={arXiv preprint arXiv:1703.02100},
  year={2017}
}

@inproceedings{mirzasoleiman2015lazier,
  title={Lazier Than Lazy Greedy.},
  author={Mirzasoleiman, Baharan and Badanidiyuru, Ashwinkumar and Karbasi, Amin and Vondr{\'a}k, Jan and Krause, Andreas},
  booktitle={AAAI},
  pages={1812--1818},
  year={2015}
}

@article{cevher2011greedy,
  title={Greedy dictionary selection for sparse representation},
  author={Cevher, Volkan and Krause, Andreas},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={5},
  number={5},
  pages={979--988},
  year={2011},
  publisher={IEEE}
}


@article{lingala2013blind,
  title={Blind compressive sensing dynamic {MRI}},
  author={Lingala, Sajan Goud and Jacob, Mathews},
  journal={IEEE transactions on medical imaging},
  volume={32},
  number={6},
  pages={1132--1145},
  year={2013},
  publisher={IEEE}
}

@article{usman2011k,
  title={k-t group sparse: {A} method for accelerating dynamic {MRI}},
  author={Usman, M and Prieto, C and Schaeffter, T and Batchelor, PG},
  journal={Magnetic Resonance in Medicine},
  volume={66},
  number={4},
  pages={1163--1176},
  year={2011},
  publisher={Wiley Online Library}
}


@article{wang2004image,
  title={Image quality assessment: from error visibility to structural similarity},
  author={Wang, Zhou and Bovik, Alan C and Sheikh, Hamid R and Simoncelli, Eero P},
  journal={IEEE transactions on image processing},
  volume={13},
  number={4},
  pages={600--612},
  year={2004},
  publisher={IEEE}
}


@article{pedersen2009k,
  title={k-t {PCA}: Temporally constrained k-t {BLAST} reconstruction using principal component analysis},
  author={Pedersen, Henrik and Kozerke, Sebastian and Ringgaard, Steffen and Nehrke, Kay and Kim, Won Yong},
  journal={Magnetic Resonance in Medicine},
  volume={62},
  number={3},
  pages={706--716},
  year={2009},
  publisher={Wiley Online Library}
}




@inproceedings{ravishankar2011adaptive,
  title={Adaptive sampling design for compressed sensing {MRI}},
  author={Ravishankar, Saiprasad and Bresler, Yoram},
  booktitle={Engineering in Medicine and Biology Society, EMBC, 2011 Annual International Conference of the IEEE},
  pages={3751--3755},
  year={2011},
  organization={IEEE}
}


@misc{fesslerMIRT,
  title={{Michigan Image Reconstruction Toolbox (MIRT})},
  author={Fessler, Jeffrey A },
  year={2006},
  note={\textit{Available at \url{https://web.eecs.umich.edu/~fessler/code/index.html} (Downloaded on the 22-12-2017)}}
}


@article{winkelmann2007optimal,
  title={An optimal radial profile order based on the Golden Ratio for time-resolved {MRI}},
  author={Winkelmann, Stefanie and Schaeffter, Tobias and Koehler, Thomas and Eggers, Holger and Doessel, Olaf},
  journal={IEEE Transactions on Medical Imaging},
  volume={26},
  number={1},
  pages={68--76},
  year={2007},
  publisher={IEEE}
}

@article{griswold2002generalized,
  title={Generalized autocalibrating partially parallel acquisitions ({GRAPPA})},
  author={Griswold, Mark A and Jakob, Peter M and Heidemann, Robin M and Nittka, Mathias and Jellus, Vladimir and Wang, Jianmin and Kiefer, Berthold and Haase, Axel},
  journal={Magnetic Resonance in Medicine},
  volume={47},
  number={6},
  pages={1202--1210},
  year={2002},
  publisher={Wiley Online Library}
}


@article{vaswani2010ls,
  title={{LS-CS}-residual ({LS-CS}): compressive sensing on least squares residual},
  author={Vaswani, Namrata},
  journal={IEEE Transactions on Signal Processing},
  volume={58},
  number={8},
  pages={4108--4120},
  year={2010},
  publisher={IEEE}
}


@article{lustig2007sparse,
  title={Sparse {MRI}: The application of compressed sensing for rapid {MR} imaging},
  author={Lustig, Michael and Donoho, David and Pauly, John M},
  journal={Magnetic Resonance in Medicine},
  volume={58},
  number={6},
  pages={1182--1195},
  year={2007},
  publisher={Wiley Online Library}
}


@article{gamper2008compressed,
  title={Compressed sensing in dynamic {MRI}},
  author={Gamper, Urs and Boesiger, Peter and Kozerke, Sebastian},
  journal={Magnetic Resonance in Medicine},
  volume={59},
  number={2},
  pages={365--373},
  year={2008},
  publisher={Wiley Online Library}
}


@article{caballero2014dictionary,
  title={Dictionary learning and time sparsity for dynamic {MR} data reconstruction},
  author={Caballero, Jose and Price, Anthony N and Rueckert, Daniel and Hajnal, Joseph V},
  journal={IEEE Transactions on Medical Imaging},
  volume={33},
  number={4},
  pages={979--994},
  year={2014},
  publisher={IEEE}
}


  @article{eksioglu2016decoupled,
  title={Decoupled algorithm for {MRI} reconstruction using nonlocal block matching model: Bm3d-{MRI}},
  author={Eksioglu, Ender M},
  journal={Journal of Mathematical Imaging and Vision},
  volume={56},
  number={3},
  pages={430--440},
  year={2016},
  publisher={Springer}
}

@article{gidel2018variational,
  title={A variational inequality perspective on generative adversarial networks},
  author={Gidel, Gauthier and Berard, Hugo and Vignoud, Ga{\"e}tan and Vincent, Pascal and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:1802.10551},
  year={2018}
}

@article{mardani_deep_2019,
	title = {Deep Generative Adversarial Neural Networks for Compressive Sensing {MRI}},
	volume = {38},
	year={2019},
	issn = {1558-254X},
	doi = {10.1109/TMI.2018.2858752},
	abstract = {Undersampled magnetic resonance image ({MRI}) reconstruction is typically an ill-posed linear inverse task. The time and resource intensive computations require tradeoffs between accuracy and speed. In addition, state-of-the-art compressed sensing ({CS}) analytics are not cognizant of the image diagnostic quality. To address these challenges, we propose a novel {CS} framework that uses generative adversarial networks ({GAN}) to model the (low-dimensional) manifold of high-quality {MR} images. Leveraging a mixture of least-squares ({LS}) {GANs} and pixel-wise l1/l2 cost, a deep residual network with skip connections is trained as the generator that learns to remove the aliasing artifacts by projecting onto the image manifold. The {LSGAN} learns the texture details, while the l1/l2 cost suppresses high-frequency noise. A discriminator network, which is a multilayer convolutional neural network ({CNN}), plays the role of a perceptual cost that is then jointly trained based on high-quality {MR} images to score the quality of retrieved images. In the operational phase, an initial aliased estimate (e.g., simply obtained by zero-filling) is propagated into the trained generator to output the desired reconstruction. This demands a very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced {MR} dataset of pediatric patients. Images rated by expert radiologists corroborate that {GANCS} retrieves higher quality images with improved fine texture details compared with conventional Wavelet-based and dictionary-learning-based {CS} schemes as well as with deep-learning-based schemes using pixel-wise training. In addition, it offers reconstruction times of under a few milliseconds, which are two orders of magnitude faster than the current state-of-the-art {CS}-{MRI} schemes.},
	pages = {167--179},
	number = {1},
	journaltitle = {{IEEE} transactions on medical imaging},
	shortjournal = {{IEEE} Trans Med Imaging},
	author = {Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y. and Vasanawala, Shreyas S. and Zaharchuk, Greg and Xing, Lei and Pauly, John M.},
	date = {2019-01},
	pmid = {30040634},
	pmcid = {PMC6542360},
	keywords = {Adrenal Glands, Algorithms, Data Compression, Databases, Factual, Humans, Knee, Magnetic Resonance Imaging, Neural Networks, Computer, Phantoms, Imaging},
	file = {Accepted Version:/home/krawczuk/Zotero/storage/ID9H3YMM/Mardani et al. - 2019 - Deep Generative Adversarial Neural Networks for Co.pdf:application/pdf},
}

@article{edupuganti_uncertainty_2020,
  title={Uncertainty quantification in deep mri reconstruction},
  author={Edupuganti, Vineet and Mardani, Morteza and Vasanawala, Shreyas and Pauly, John},
  journal={IEEE Transactions on Medical Imaging},
  volume={40},
  number={1},
  pages={239--250},
  year={2020},
  publisher={IEEE}
}

@misc{ramzi2020denoising,
	title={Denoising Score-Matching for Uncertainty Quantification in Inverse Problems}, 
	author={Zaccharie Ramzi and Benjamin Remy and Francois Lanusse and Jean-Luc Starck and Philippe Ciuciu},
	year={2020},
	eprint={2011.08698},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}
@inproceedings{
	arora2018do,
	title={Do {GAN}s learn the distribution? Some Theory and Empirics},
	author={Sanjeev Arora and Andrej Risteski and Yi Zhang},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=BJehNfW0-},
}
@misc{schonfeld2020unet,
	title={A U-Net Based Discriminator for Generative Adversarial Networks}, 
	author={Edgar Schönfeld and Bernt Schiele and Anna Khoreva},
	year={2020},
	eprint={2002.12655},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}
@article{che2016mode,
	title={Mode regularized generative adversarial networks},
	author={Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
	journal={arXiv preprint arXiv:1612.02136},
	year={2016}
}
@InProceedings{Xie_2018_ECCV_Workshops,
	author = {Xie, Yiting and Richmond, David},
	title = {Pre-training on Grayscale ImageNet Improves Medical Image Classification},
	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
	month = {September},
	year = {2018}
}
@inproceedings{NEURIPS2019_b53477c2,
	author = {Osawa, Kazuki and Swaroop, Siddharth and Khan, Mohammad Emtiyaz E and Jain, Anirudh and Eschenhagen, Runa and Turner, Richard E and Yokota, Rio},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {4287--4299},
	publisher = {Curran Associates, Inc.},
	title = {Practical Deep Learning with Bayesian Principles},
	url = {https://proceedings.neurips.cc/paper/2019/file/b53477c2821c1bf0da5d40e57b870d35-Paper.pdf},
	volume = {32},
	year = {2019}
}

@inproceedings{lin_pacgan_2018,
 author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PacGAN: The power of two samples in generative adversarial networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{chen_sequential_nodate,
	title = {Sequential Information Maximization: When is Greedy Near-optimal?},
	abstract = {Optimal information gathering is a central challenge in machine learning and science in general. A common objective that quantiﬁes the usefulness of observations is Shannon’s mutual information, deﬁned w.r.t. a probabilistic model. Greedily selecting observations that maximize the mutual information is the method of choice in numerous applications, ranging from Bayesian experimental design to automated diagnosis, to active learning in Bayesian models. Despite its importance and widespread use in applications, little is known about the theoretical properties of sequential information maximization, in particular under noisy observations. In this paper, we analyze the widely used greedy policy for this task, and identify problem instances where it provides provably near-maximal utility, even in the challenging setting of persistent noise. Our results depend on a natural separability condition associated with a channel injecting noise into the observations. We also identify examples where this separability parameter is necessary in the bound: if it is too small, then the greedy policy fails to select informative tests.},
	pages = {26},
	year={2015},
	author = {Chen, Yuxin and Hassani, S Hamed and Karbasi, Amin and Krause, Andreas},
	langid = {english},
	file = {Chen et al. - Sequential Information Maximization When is Greed.pdf:/home/krawczuk/Zotero/storage/PWHHFUBR/Chen et al. - Sequential Information Maximization When is Greed.pdf:application/pdf},
}
@article{grill_bootstrap_2020,
	title = {Bootstrap your own latent: A new approach to self-supervised Learning},
	url = {http://arxiv.org/abs/2006.07733},
	year={2020},
	shorttitle = {Bootstrap your own latent},
	abstract = {We introduce Bootstrap Your Own Latent ({BYOL}), a new approach to self-supervised image representation learning. {BYOL} relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, {BYOL} achieves a new state of the art without them. {BYOL} reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on {ImageNet} using a linear evaluation with a {ResNet}-50 architecture and \$79.6{\textbackslash}\%\$ with a larger {ResNet}. We show that {BYOL} performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on {GitHub}.},
	journaltitle = {{arXiv}:2006.07733 [cs, stat]},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	urldate = {2020-09-11},
	date = {2020-09-10},
	eprinttype = {arxiv},
	eprint = {2006.07733},
	note = {86 citations (Semantic Scholar/{arXiv}) [2020-12-13]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, representations, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/krawczuk/Zotero/storage/XVEMM9RL/2006.html:text/html;Grill et al_2020_Bootstrap your own latent.pdf:/home/krawczuk/Zotero/storage/ZQGKDDX6/Grill et al_2020_Bootstrap your own latent.pdf:application/pdf;arXiv.org Snapshot:/home/krawczuk/Zotero/storage/NVXV64P2/2006.html:text/html;Grill et al_2020_Bootstrap your own latent.pdf:/home/krawczuk/Zotero/storage/NW6GGCVA/Grill et al_2020_Bootstrap your own latent.pdf:application/pdf;arXiv Fulltext PDF:/home/krawczuk/Zotero/storage/DY3EQMPU/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf:application/pdf;arXiv.org Snapshot:/home/krawczuk/Zotero/storage/FC99LRPA/2006.html:text/html;arXiv Fulltext PDF:/home/krawczuk/Zotero/storage/FA6V53F2/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf:application/pdf;arXiv.org Snapshot:/home/krawczuk/Zotero/storage/JSVD6UHM/2006.html:text/html},
}

@article{zhang_unreasonable_2018,
	title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
	url = {http://arxiv.org/abs/1801.03924},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as {PSNR} and {SSIM}, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the {VGG} network trained on {ImageNet} classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to {ImageNet}-trained {VGG} features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	journaltitle = {{arXiv}:1801.03924 [cs]},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	urldate = {2020-09-17},
	date = {2018-04-10},
	year={2018},
	eprinttype = {arxiv},
	eprint = {1801.03924},
	note = {1004 citations (Semantic Scholar/{arXiv}) [2020-12-13]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv.org Snapshot:/home/krawczuk/Zotero/storage/5V6972US/1801.html:text/html;Zhang et al_2018_The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf:/home/krawczuk/Zotero/storage/GZJE4GA8/Zhang et al_2018_The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf:application/pdf},
}
@article{song_score-based_2020,
	title = {Score-Based Generative Modeling through Stochastic Differential Equations},
	url = {http://arxiv.org/abs/2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation ({SDE}) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time {SDE} that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time {SDE} depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical {SDE} solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and score-based generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time {SDE}. We also derive an equivalent neural {ODE} that samples from the same distribution as the {SDE}, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on {CIFAR}-10 with an Inception score of 9.89 and {FID} of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of \$1024 {\textbackslash}times 1024\$ images for the first time from a score-based generative model.},
	journaltitle = {{arXiv}:2011.13456 [cs, stat]},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	urldate = {2021-01-05},
	date = {2020-11-26},
	eprinttype = {arxiv},
	eprint = {2011.13456},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/krawczuk/Zotero/storage/SQI3M3C8/Song et al. - 2020 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf;arXiv.org Snapshot:/home/krawczuk/Zotero/storage/MTZGPILN/2011.html:text/html},
}

@article{denker_conditional_2020,
	title = {Conditional Normalizing Flows for Low-Dose Computed Tomography Image Reconstruction},
	url = {http://arxiv.org/abs/2006.06270},
	abstract = {Image reconstruction from computed tomography ({CT}) measurement is a challenging statistical inverse problem since a high-dimensional conditional distribution needs to be estimated. Based on training data obtained from high-quality reconstructions, we aim to learn a conditional density of images from noisy low-dose {CT} measurements. To tackle this problem, we propose a hybrid conditional normalizing flow, which integrates the physical model by using the filtered back-projection as conditioner. We evaluate our approach on a low-dose {CT} benchmark and demonstrate superior performance in terms of structural similarity of our flow-based method compared to other deep learning based approaches.},
	journaltitle = {{arXiv}:2006.06270 [eess]},
	author = {Denker, Alexander and Schmidt, Maximilian and Leuschner, Johannes and Maass, Peter and Behrmann, Jens},
	urldate = {2021-01-05},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2006.06270},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/krawczuk/Zotero/storage/B2ZAJTQE/Denker et al. - 2020 - Conditional Normalizing Flows for Low-Dose Compute.pdf:application/pdf;arXiv.org Snapshot:/home/krawczuk/Zotero/storage/CXTX89ZJ/2006.html:text/html},
}

@article{whang_compressed_2020,
	title = {Compressed Sensing with Invertible Generative Models and Dependent Noise},
	url = {http://arxiv.org/abs/2003.08089},
	abstract = {We study image inverse problems with invertible generative priors, specifically normalizing flow models. Our formulation views the solution as the maximum a posteriori ({MAP}) estimate of the image given the measurements. Our general formulation allows for any differentiable noise model with long-range dependencies as well as non-linear differentiable forward operators. We establish theoretical recovery guarantees for denoising and compressed sensing under our framework. We also empirically validate our method on various inverse problems including compressed sensing with quantized measurements and denoising with highly structured noise patterns.},
	journaltitle = {{arXiv}:2003.08089 [cs, math, stat]},
	author = {Whang, Jay and Lei, Qi and Dimakis, Alexandros G.},
	urldate = {2021-01-05},
	date = {2020-07-29},
	eprinttype = {arxiv},
	eprint = {2003.08089},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/krawczuk/Zotero/storage/DN3WWK3S/Whang et al. - 2020 - Compressed Sensing with Invertible Generative Mode.pdf:application/pdf;arXiv.org Snapshot:/home/krawczuk/Zotero/storage/MK6ANTS9/2003.html:text/html},
}

@online{anAEOTGANTrainingGANs2020,
  title = {{{AE}}-{{OT}}-{{GAN}}: {{Training GANs}} from Data Specific Latent Distribution},
  shorttitle = {{{AE}}-{{OT}}-{{GAN}}},
  author = {An, Dongsheng and Guo, Yang and Zhang, Min and Qi, Xin and Lei, Na and Yau, Shing-Tung and Gu, Xianfeng},
  date = {2020-01-27},
  url = {http://arxiv.org/abs/2001.03698},
  urldate = {2021-01-06},
  abstract = {Though generative adversarial networks (GANs) areprominent models to generate realistic and crisp images,they often encounter the mode collapse problems and arehard to train, which comes from approximating the intrinsicdiscontinuous distribution transform map with continuousDNNs. The recently proposed AE-OT model addresses thisproblem by explicitly computing the discontinuous distribu-tion transform map through solving a semi-discrete optimaltransport (OT) map in the latent space of the autoencoder.However the generated images are blurry. In this paper, wepropose the AE-OT-GAN model to utilize the advantages ofthe both models: generate high quality images and at thesame time overcome the mode collapse/mixture problems.Specifically, we first faithfully embed the low dimensionalimage manifold into the latent space by training an autoen-coder (AE). Then we compute the optimal transport (OT)map that pushes forward the uniform distribution to the la-tent distribution supported on the latent manifold. Finally,our GAN model is trained to generate high quality imagesfrom the latent distribution, the distribution transform mapfrom which to the empirical data distribution will be con-tinuous. The paired data between the latent code and thereal images gives us further constriction about the generator.Experiments on simple MNIST dataset and complex datasetslike Cifar-10 and CelebA show the efficacy and efficiency ofour proposed method.},
  archivePrefix = {arXiv},
  eprint = {2001.03698},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/KRKXCV89/An et al. - 2020 - AE-OT-GAN Training GANs from data specific latent.pdf;/home/krawczuk/Zotero/storage/B68CLGKC/2001.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,GAN},
  primaryClass = {cs, eess}
}

@inproceedings{
baiApproximabilityDiscriminatorsImplies2019a,
title={Approximability of Discriminators Implies Diversity in {GAN}s},
author={Yu Bai and Tengyu Ma and Andrej Risteski},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJfW5oA5KQ},
}

@online{bangMGGANSolvingMode2018,
  title = {{{MGGAN}}: {{Solving Mode Collapse}} Using {{Manifold Guided Training}}},
  shorttitle = {{{MGGAN}}},
  author = {Bang, Duhyeon and Shim, Hyunjung},
  date = {2018-04-12},
  url = {http://arxiv.org/abs/1804.04391},
  urldate = {2021-01-06},
  abstract = {Mode collapse is a critical problem in training generative adversarial networks. To alleviate mode collapse, several recent studies introduce new objective functions, network architectures or alternative training schemes. However, their achievement is often the result of sacrificing the image quality. In this paper, we propose a new algorithm, namely a manifold guided generative adversarial network (MGGAN), which leverages a guidance network on existing GAN architecture to induce generator learning all modes of data distribution. Based on extensive evaluations, we show that our algorithm resolves mode collapse without losing image quality. In particular, we demonstrate that our algorithm is easily extendable to various existing GANs. Experimental analysis justifies that the proposed algorithm is an effective and efficient tool for training GANs.},
  archivePrefix = {arXiv},
  eprint = {1804.04391},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/ZD22XLDB/Bang and Shim - 2018 - MGGAN Solving Mode Collapse using Manifold Guided.pdf;/home/krawczuk/Zotero/storage/223KB7FZ/1804.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,GAN,mode collapse},
  primaryClass = {cs}
}

@online{berthelotBeganBoundaryEquilibrium2017,
  title = {Began: {{Boundary}} Equilibrium Generative Adversarial Networks},
  shorttitle = {Began},
  author = {Berthelot, David and Schumm, Thomas and Metz, Luke},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1703.10717},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/TLPPIUW6/Berthelot et al. - 2017 - Began Boundary equilibrium generative adversarial.pdf;/home/krawczuk/Zotero/storage/E3AHM43Q/1703.html},
  keywords = {GAN}
}

@online{coleUnsupervisedMRIReconstruction2020,
  title = {Unsupervised {{MRI Reconstruction}} with {{Generative Adversarial Networks}}},
  author = {Cole, Elizabeth K. and Pauly, John M. and Vasanawala, Shreyas S. and Ong, Frank},
  date = {2020-08-29},
  url = {http://arxiv.org/abs/2008.13065},
  urldate = {2021-01-06},
  abstract = {Deep learning-based image reconstruction methods have achieved promising results across multiple MRI applications. However, most approaches require large-scale fully-sampled ground truth data for supervised training. Acquiring fully-sampled data is often either difficult or impossible, particularly for dynamic contrast enhancement (DCE), 3D cardiac cine, and 4D flow. We present a deep learning framework for MRI reconstruction without any fully-sampled data using generative adversarial networks. We test the proposed method in two scenarios: retrospectively undersampled fast spin echo knee exams and prospectively undersampled abdominal DCE. The method recovers more anatomical structure compared to conventional methods.},
  archivePrefix = {arXiv},
  eprint = {2008.13065},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/AKK8TPJZ/Cole et al. - 2020 - Unsupervised MRI Reconstruction with Generative Ad.pdf;/home/krawczuk/Zotero/storage/5FVFJT6R/2008.html},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,GAN,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@online{eideSampleWeightingExplanation2020,
  title = {Sample Weighting as an Explanation for Mode Collapse in Generative Adversarial Networks},
  author = {Eide, Aksel Wilhelm Wold and Solberg, Eilif and Kåsen, Ingebjørg},
  date = {2020-10-05},
  url = {http://arxiv.org/abs/2010.02035},
  urldate = {2021-01-06},
  abstract = {Generative adversarial networks were introduced with a logistic MiniMax cost formulation, which normally fails to train due to saturation, and a Non-Saturating reformulation. While addressing the saturation problem, NS-GAN also inverts the generator's sample weighting, implicitly shifting emphasis from higher-scoring to lower-scoring samples when updating parameters. We present both theory and empirical results suggesting that this makes NS-GAN prone to mode dropping. We design MM-nsat, which preserves MM-GAN sample weighting while avoiding saturation by rescaling the MM-GAN minibatch gradient such that its magnitude approximates NS-GAN's gradient magnitude. MM-nsat has qualitatively different training dynamics, and on MNIST and CIFAR-10 it is stronger in terms of mode coverage, stability and FID. While the empirical results for MM-nsat are promising and favorable also in comparison with the LS-GAN and Hinge-GAN formulations, our main contribution is to show how and why NS-GAN's sample weighting causes mode dropping and training collapse.},
  archivePrefix = {arXiv},
  eprint = {2010.02035},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/6NJHGAQ2/Eide et al. - 2020 - Sample weighting as an explanation for mode collap.pdf;/home/krawczuk/Zotero/storage/8UCPB9S5/2010.html},
  keywords = {Computer Science - Machine Learning,GAN,mode collapse,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{feiziUnderstandingGANsLQG2018,
  title = {Understanding {{GANs}}: The {{LQG Setting}}},
  shorttitle = {Understanding {{GANs}}},
  author = {Feizi, Soheil and Farnia, Farzan and Ginart, Tony and Tse, David},
  date = {2018-10-22},
  url = {http://arxiv.org/abs/1710.10793},
  urldate = {2021-01-06},
  abstract = {Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. In this paper, we aim to provide an understanding of some of the basic issues surrounding GANs including their formulation, generalization and stability on a simple benchmark where the data has a high-dimensional Gaussian distribution. Even in this simple benchmark, the GAN problem has not been well-understood as we observe that existing state-of-the-art GAN architectures may fail to learn a proper generative distribution owing to (1) stability issues (i.e., convergence to bad local solutions or not converging at all), (2) approximation issues (i.e., having improper global GAN optimizers caused by inappropriate GAN's loss functions), and (3) generalizability issues (i.e., requiring large number of samples for training). In this setup, we propose a GAN architecture which recovers the maximum-likelihood solution and demonstrates fast generalization. Moreover, we analyze global stability of different computational approaches for the proposed GAN optimization and highlight their pros and cons. Finally, we outline an extension of our model-based approach to design GANs in more complex setups than the considered Gaussian benchmark.},
  archivePrefix = {arXiv},
  eprint = {1710.10793},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/6LDSUFLF/Feizi et al. - 2018 - Understanding GANs the LQG Setting.pdf;/home/krawczuk/Zotero/storage/F84C43P5/1710.html},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,GAN,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{husseinImageAdaptiveGANBased2020,
  title = {Image-{{Adaptive GAN Based Reconstruction}}},
  author = {Hussein, Shady Abu and Tirer, Tom and Giryes, Raja},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  pages = {3121--3129},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.5708},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5708},
  urldate = {2021-01-06},
  file = {/home/krawczuk/Zotero/storage/JFRXWYR6/Hussein et al. - 2020 - Image-Adaptive GAN Based Reconstruction.pdf;/home/krawczuk/Zotero/storage/SQSZ7PRA/5708.html},
  issue = {04},
  keywords = {GAN},
  langid = {english},
}

@online{jolicoeur-martineauAdversarialScoreMatching2020,
  title = {Adversarial Score Matching and Improved Sampling for Image Generation},
  author = {Jolicoeur-Martineau, Alexia and Piché-Taillefer, Rémi and des Combes, Rémi Tachet and Mitliagkas, Ioannis},
  date = {2020-10-10},
  url = {http://arxiv.org/abs/2009.05475},
  urldate = {2021-01-06},
  abstract = {Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Fr\textbackslash 'echet Inception Distance, a standard metric for generative models. We show that this apparent gap vanishes when denoising the final Langevin samples using the score network. In addition, we propose two improvements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10.},
  archivePrefix = {arXiv},
  eprint = {2009.05475},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/9V4H8C4X/Jolicoeur-Martineau et al. - 2020 - Adversarial score matching and improved sampling f.pdf;/home/krawczuk/Zotero/storage/6B9BFLW8/2009.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,GAN,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{jolicoeur-martineauRELATIVISTICDISCRIMINATORKEY2019,
  title = {{{THE RELATIVISTIC DISCRIMINATOR}}: {{A KEY ELEMENT MISSING FROM STANDARD GAN}}},
  author = {Jolicoeur-Martineau, Alexia},
  date = {2019},
  pages = {26},
  abstract = {In standard generative adversarial network (SGAN), the discriminator D estimates the probability that the input data is real. The generator G is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) SGAN would be more similar to integral probability metric (IPM) GANs.},
  file = {/home/krawczuk/Zotero/storage/R2WTKTFZ/Jolicoeur-Martineau - 2019 - THE RELATIVISTIC DISCRIMINATOR A KEY ELEMENT MISS.pdf},
  keywords = {GAN},
  langid = {english}
}

@inproceedings{jolicoeur-martineauRelativisticFDivergences2020,
  title = {On {{Relativistic}} F-{{Divergences}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Jolicoeur-Martineau, Alexia},
  date = {2020-11-21},
  pages = {4931--4939},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/jolicoeur-martineau20a.html},
  urldate = {2021-01-06},
  abstract = {We take a more rigorous look at Relativistic Generative Adversarial Networks (RGANs) and prove that the objective function of the discriminator is a statistical divergence for any concave function ...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {/home/krawczuk/Zotero/storage/JJ37CW8M/Jolicoeur-Martineau - 2020 - On Relativistic f-Divergences.pdf},
  keywords = {GAN},
  langid = {english}
}

@online{kodaliConvergenceStabilityGANs2017e,
  title = {On {{Convergence}} and {{Stability}} of {{GANs}}},
  author = {Kodali, Naveen and Abernethy, Jacob and Hays, James and Kira, Zsolt},
  date = {2017-12-10},
  url = {http://arxiv.org/abs/1705.07215},
  urldate = {2021-01-06},
  abstract = {We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.},
  archivePrefix = {arXiv},
  eprint = {1705.07215},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/I69KUXZR/Kodali et al. - 2017 - On Convergence and Stability of GANs.pdf;/home/krawczuk/Zotero/storage/D75GR3CI/1705.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,GAN,mode collapse},
  primaryClass = {cs}
}

@inproceedings{leeHarmonizingMaximumLikelihood2018,
  title = {Harmonizing {{Maximum Likelihood}} with {{GANs}} for {{Multimodal Conditional Generation}}},
  author = {Lee, Soochan and Ha, Junsoo and Kim, Gunhee},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=HJxyAjRcFX},
  urldate = {2021-01-06},
  abstract = {We prove that the mode collapse in conditional GANs is largely attributed to a mismatch between reconstruction loss and GAN loss and introduce a set of novel loss functions as alternatives for...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  file = {/home/krawczuk/Zotero/storage/4E75C7Q8/Lee et al. - 2018 - Harmonizing Maximum Likelihood with GANs for Multi.pdf},
  keywords = {GAN},
  langid = {english}
}

@inproceedings{leeInfoMaxGANImprovedAdversarial2021,
  title = {{{InfoMax}}-{{GAN}}: {{Improved Adversarial Image Generation}} via {{Information Maximization}} and {{Contrastive Learning}}},
  shorttitle = {{{InfoMax}}-{{GAN}}},
  author = {Lee, Kwot Sin and Tran, Ngoc-Trung and Cheung, Ngai-Man},
  date = {2021},
  pages = {3942--3952},
  url = {https://openaccess.thecvf.com/content/WACV2021/html/Lee_InfoMax-GAN_Improved_Adversarial_Image_Generation_via_Information_Maximization_and_Contrastive_WACV_2021_paper.html},
  urldate = {2021-01-06},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  file = {/home/krawczuk/Zotero/storage/NGI8VNXZ/Lee et al. - 2021 - InfoMax-GAN Improved Adversarial Image Generation.pdf;/home/krawczuk/Zotero/storage/JDFGWCD7/Lee_InfoMax-GAN_Improved_Adversarial_Image_Generation_via_Information_Maximization_and_Contrast.html},
  keywords = {GAN},
  langid = {english}
}

@article{liTacklingModeCollapse2021,
  title = {Tackling Mode Collapse in Multi-Generator {{GANs}} with Orthogonal Vectors},
  author = {Li, Wei and Fan, Li and Wang, Zhenyu and Ma, Chao and Cui, Xiaohui},
  date = {2021-02-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {110},
  pages = {107646},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2020.107646},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320320304490},
  urldate = {2021-01-06},
  abstract = {Generative Adversarial Networks (GANs) have been widely used to generate realistic-looking instances. However, training robust GAN is a non-trivial task due to the problem of mode collapse. Although many GAN variants are proposed to overcome this problem, they have limitations. Those existing studies either generate identical instances or result in negative gradients during training. In this paper, we propose a new approach to training GAN to overcome mode collapse by employing a set of generators, an encoder and a discriminator. A new minimax formula is proposed to simultaneously train all components in a similar spirit to vanilla GAN. The orthogonal vector strategy is employed to guide multiple generators to learn different information in a complementary manner. In this way, we term our approach Multi-Generator Orthogonal GAN (MGO-GAN). Specifically, the synthetic data produced by those generators are fed into the encoder to obtain feature vectors. The orthogonal value is calculated between any two feature vectors, which loyally reflects the correlation between vectors. Such a correlation indicates how different information has been learnt by generators. The lower the orthogonal value is, the more different information the generators learn. We minimize the orthogonal value along with minimizing the generator loss through back-propagation in the training of GAN. The orthogonal value is integrated with the original generator loss to jointly update the corresponding generator’s parameters. We conduct extensive experiments utilizing MNIST, CIFAR10 and CelebA datasets to demonstrate the significant performance improvement of MGO-GAN in terms of generated data quality and diversity at different resolutions.},
  file = {/home/krawczuk/Zotero/storage/KJZ5NTAC/Li et al. - 2021 - Tackling mode collapse in multi-generator GANs wit.pdf;/home/krawczuk/Zotero/storage/67DYC6Q5/S0031320320304490.html},
  keywords = {GAN,GANs,Minimax formula,mode collapse,Mode collapse,Multiple generators,Orthogonal vectors},
  langid = {english}
}

@inproceedings{loizouStochasticHamiltonianGradient2020,
  title = {Stochastic {{Hamiltonian Gradient Methods}} for {{Smooth Games}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Loizou, Nicolas and Berard, Hugo and Jolicoeur-Martineau, Alexia and Vincent, Pascal and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  date = {2020-11-21},
  pages = {6370--6381},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/loizou20a.html},
  urldate = {2021-01-06},
  abstract = {The success of adversarial formulations in machine learning has brought renewed motivation for smooth games. In this work, we focus on the class of stochastic Hamiltonian methods and provide the fi...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {/home/krawczuk/Zotero/storage/UUMRART3/Loizou et al. - 2020 - Stochastic Hamiltonian Gradient Methods for Smooth.pdf;/home/krawczuk/Zotero/storage/LXYSVFYF/loizou20a.html},
  keywords = {GAN},
  langid = {english}
}

@online{marusakiCapsuleGANUsing2020,
  title = {Capsule {{GAN Using Capsule Network}} for {{Generator Architecture}}},
  author = {Marusaki, Kanako and Watanabe, Hiroshi},
  date = {2020-03-18},
  url = {http://arxiv.org/abs/2003.08047},
  urldate = {2021-01-06},
  abstract = {This paper presents Capsule GAN, a Generative adversarial network using Capsule Network not only in the discriminator but also in the generator. Recently, Generative adversarial networks (GANs) has been intensively studied. However, generating images by GANs is difficult. Therefore, GANs sometimes generate poor quality images. These GANs use convolutional neural networks (CNNs). However, CNNs have the defect that the relational information between features of the image may be lost. Capsule Network, proposed by Hinton in 2017, overcomes the defect of CNNs. Capsule GAN reported previously uses Capsule Network in the discriminator. However, instead of using Capsule Network, Capsule GAN reported in previous studies uses CNNs in generator architecture like DCGAN. This paper introduces two approaches to use Capsule Network in the generator. One is to use DigitCaps layer from the discriminator as the input to the generator. DigitCaps layer is the output layer of Capsule Network. It has the features of the input images of the discriminator. The other is to use the reverse operation of recognition process in Capsule Network in the generator. We compare Capsule GAN proposed in this paper with conventional GAN using CNN and Capsule GAN which uses Capsule Network in the discriminator only. The datasets are MNIST, Fashion-MNIST and color images. We show that Capsule GAN outperforms the GAN using CNN and the GAN using Capsule Network in the discriminator only. The architecture of Capsule GAN proposed in this paper is a basic architecture using Capsule Network. Therefore, we can apply the existing improvement techniques for GANs to Capsule GAN.},
  archivePrefix = {arXiv},
  eprint = {2003.08047},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/9AVQPTH7/Marusaki and Watanabe - 2020 - Capsule GAN Using Capsule Network for Generator Ar.pdf;/home/krawczuk/Zotero/storage/S5U9KIVQ/2003.html},
  keywords = {68T05,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,GAN},
  primaryClass = {cs, eess}
}

@article{metzLearnedOptimizersThat,
  title = {Learned Optimizers That Outperform {{SGD}} on Wall-Clock and Test Loss},
  author = {Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Freeman, Daniel and Sohl-dickstein, Jascha},
  pages = {16},
  abstract = {Deep learning has shown that learned functions can dramatically outperform handdesigned functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization faster than learning rate tuned first-order methods. Moreover, by training the optimizer against validation loss (as opposed to training loss), we are able to learn optimizers that train networks to better generalization than first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to learning rate tuned first-order methods, and with an improvement in validation loss.},
  file = {/home/krawczuk/Zotero/storage/W3BDWPAI/Metz et al. - Learned optimizers that outperform SGD on wall-clo.pdf},
  keywords = {GAN},
  langid = {english},
  year={2018}
}

@article{metzUnderstandingCorrectingPathologiesa,
  title = {Understanding and Correcting Pathologies in the Training of Learned Optimizers},
  author = {Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Freeman, C Daniel and Sohl-Dickstein, Jascha},
  pages = {10},
  abstract = {Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wallclock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process resulting in gradients that are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance, allowing us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss.},
  file = {/home/krawczuk/Zotero/storage/B9ICISGV/Metz et al. - Understanding and correcting pathologies in the tr.pdf},
  keywords = {GAN},
  langid = {english},
  year={2018},
}

@online{metzUnrolledGenerativeAdversarial2017,
  title = {Unrolled {{Generative Adversarial Networks}}},
  author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  date = {2017-05-12},
  url = {http://arxiv.org/abs/1611.02163},
  urldate = {2021-01-06},
  abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
  archivePrefix = {arXiv},
  eprint = {1611.02163},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/2ZG28LP2/Metz et al. - 2017 - Unrolled Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/8HZX82V7/1611.html},
  keywords = {Computer Science - Machine Learning,GAN,mode collapse,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{mongaAlgorithmUnrollingInterpretable2020,
  title = {Algorithm {{Unrolling}}: {{Interpretable}}, {{Efficient Deep Learning}} for {{Signal}} and {{Image Processing}}},
  shorttitle = {Algorithm {{Unrolling}}},
  author = {Monga, Vishal and Li, Yuelong and Eldar, Yonina C.},
  date = {2020-08-07},
  url = {http://arxiv.org/abs/1912.10557},
  urldate = {2021-01-06},
  abstract = {Deep neural networks provide unprecedented performance gains in many real world problems in signal and image processing. Despite these gains, future development and practical deployment of deep networks is hindered by their blackbox nature, i.e., lack of interpretability, and by the need for very large training sets. An emerging technique called algorithm unrolling or unfolding offers promise in eliminating these issues by providing a concrete and systematic connection between iterative algorithms that are used widely in signal processing and deep neural networks. Unrolling methods were first proposed to develop fast neural network approximations for sparse coding. More recently, this direction has attracted enormous attention and is rapidly growing both in theoretic investigations and practical applications. The growing popularity of unrolled deep networks is due in part to their potential in developing efficient, high-performance and yet interpretable network architectures from reasonable size training sets. In this article, we review algorithm unrolling for signal and image processing. We extensively cover popular techniques for algorithm unrolling in various domains of signal and image processing including imaging, vision and recognition, and speech processing. By reviewing previous works, we reveal the connections between iterative algorithms and neural networks and present recent theoretical results. Finally, we provide a discussion on current limitations of unrolling and suggest possible future research directions.},
  archivePrefix = {arXiv},
  eprint = {1912.10557},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/RGDNE74B/Monga et al. - 2020 - Algorithm Unrolling Interpretable, Efficient Deep.pdf;/home/krawczuk/Zotero/storage/4T2I3HLA/1912.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing,GAN},
  primaryClass = {cs, eess}
}

@online{nagarajanGradientDescentGAN2018a,
  title = {Gradient Descent {{GAN}} Optimization Is Locally Stable},
  author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
  date = {2018-01-13},
  url = {http://arxiv.org/abs/1706.04156},
  urldate = {2021-01-06},
  abstract = {Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the "gradient descent" form of GAN optimization i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still \textbackslash emph\{locally asymptotically stable\} for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which \textbackslash emph\{is\} able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.},
  archivePrefix = {arXiv},
  eprint = {1706.04156},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/E784DW2F/Nagarajan and Kolter - 2018 - Gradient descent GAN optimization is locally stabl.pdf;/home/krawczuk/Zotero/storage/UM452I3K/1706.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,GAN,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@online{pantComplexityControlledGenerative2020,
  title = {Complexity {{Controlled Generative Adversarial Networks}}},
  author = {Pant, Himanshu and Jayadeva and Soman, Sumit},
  date = {2020-11-20},
  url = {http://arxiv.org/abs/2011.10223},
  urldate = {2021-01-06},
  abstract = {One of the issues faced in training Generative Adversarial Nets (GANs) and their variants is the problem of mode collapse, wherein the training stability in terms of the generative loss increases as more training data is used. In this paper, we propose an alternative architecture via the Low-Complexity Neural Network (LCNN), which attempts to learn models with low complexity. The motivation is that controlling model complexity leads to models that do not overfit the training data. We incorporate the LCNN loss function for GANs, Deep Convolutional GANs (DCGANs) and Spectral Normalized GANs (SNGANs), in order to develop hybrid architectures called the LCNN-GAN, LCNN-DCGAN and LCNN-SNGAN respectively. On various large benchmark image datasets, we show that the use of our proposed models results in stable training while avoiding the problem of mode collapse, resulting in better training stability. We also show how the learning behavior can be controlled by a hyperparameter in the LCNN functional, which also provides an improved inception score.},
  archivePrefix = {arXiv},
  eprint = {2011.10223},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/EX8PC7Z3/Pant et al. - 2020 - Complexity Controlled Generative Adversarial Netwo.pdf;/home/krawczuk/Zotero/storage/GT7JS5BH/2011.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,GAN},
  primaryClass = {cs}
}

@article{parkBEGANV3Avoiding2020,
  title = {{{BEGAN}} v3: {{Avoiding Mode Collapse}} in {{GANs Using Variational Inference}}},
  shorttitle = {{{BEGAN}} V3},
  author = {Park, Sung-Wook and Huh, Jun-Ho and Kim, Jong-Chan},
  date = {2020-04},
  journaltitle = {Electronics},
  volume = {9},
  pages = {688},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/electronics9040688},
  url = {https://www.mdpi.com/2079-9292/9/4/688},
  urldate = {2021-01-06},
  abstract = {In the field of deep learning, the generative model did not attract much attention until GANs (generative adversarial networks) appeared. In 2014, Google’s Ian Goodfellow proposed a generative model called GANs. GANs use different structures and objective functions from the existing generative model. For example, GANs use two neural networks: a generator that creates a realistic image, and a discriminator that distinguishes whether the input is real or synthetic. If there are no problems in the training process, GANs can generate images that are difficult even for experts to distinguish in terms of authenticity. Currently, GANs are the most researched subject in the field of computer vision, which deals with the technology of image style translation, synthesis, and generation, and various models have been unveiled. The issues raised are also improving one by one. In image synthesis, BEGAN (Boundary Equilibrium Generative Adversarial Network), which outperforms the previously announced GANs, learns the latent space of the image, while balancing the generator and discriminator. Nonetheless, BEGAN also has a mode collapse wherein the generator generates only a few images or a single one. Although BEGAN-CS (Boundary Equilibrium Generative Adversarial Network with Constrained Space), which was improved in terms of loss function, was introduced, it did not solve the mode collapse. The discriminator structure of BEGAN-CS is AE (AutoEncoder), which cannot create a particularly useful or structured latent space. Compression performance is not good either. In this paper, this characteristic of AE is considered to be related to the occurrence of mode collapse. Thus, we used VAE (Variational AutoEncoder), which added statistical techniques to AE. As a result of the experiment, the proposed model did not cause mode collapse but converged to a better state than BEGAN-CS.},
  file = {/home/krawczuk/Zotero/storage/WSJLVAJW/Park et al. - 2020 - BEGAN v3 Avoiding Mode Collapse in GANs Using Var.pdf;/home/krawczuk/Zotero/storage/BFHEIMZM/688.html;/home/krawczuk/Zotero/storage/UPJ3CW7J/688.html},
  issue = {4},
  keywords = {artificial intelligence,boundary equilibrium generative adversarial networks,computer vision,deep learning,GAN,generative adversarial networks,mode collapse,variational inference},
  langid = {english},
  number = {4}
}

@online{qinTrainingGenerativeAdversarial2020,
  title = {Training {{Generative Adversarial Networks}} by {{Solving Ordinary Differential Equations}}},
  author = {Qin, Chongli and Wu, Yan and Springenberg, Jost Tobias and Brock, Andrew and Donahue, Jeff and Lillicrap, Timothy P. and Kohli, Pushmeet},
  date = {2020-11-28},
  url = {http://arxiv.org/abs/2010.15040},
  urldate = {2021-01-06},
  abstract = {The instability of Generative Adversarial Network (GAN) training has frequently been attributed to gradient descent. Consequently, recent methods have aimed to tailor the models and training procedures to stabilise the discrete updates. In contrast, we study the continuous-time dynamics induced by GAN training. Both theory and toy experiments suggest that these dynamics are in fact surprisingly stable. From this perspective, we hypothesise that instabilities in training GANs arise from the integration error in discretising the continuous dynamics. We experimentally verify that well-known ODE solvers (such as Runge-Kutta) can stabilise training - when combined with a regulariser that controls the integration error. Our approach represents a radical departure from previous methods which typically use adaptive optimisation and stabilisation techniques that constrain the functional space (e.g. Spectral Normalisation). Evaluation on CIFAR-10 and ImageNet shows that our method outperforms several strong baselines, demonstrating its efficacy.},
  archivePrefix = {arXiv},
  eprint = {2010.15040},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/DIWZ6SCC/Qin et al. - 2020 - Training Generative Adversarial Networks by Solvin.pdf;/home/krawczuk/Zotero/storage/YSD2RLND/2010.html},
  keywords = {Computer Science - Machine Learning,GAN,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{quanCompressedSensingMRI2018,
  title = {Compressed {{Sensing MRI Reconstruction}} Using a {{Generative Adversarial Network}} with a {{Cyclic Loss}}},
  author = {Quan, Tran Minh and Nguyen-Duc, Thanh and Jeong, Won-Ki},
  date = {2018-06},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {37},
  pages = {1488--1497},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2018.2820120},
  url = {http://arxiv.org/abs/1709.00753},
  urldate = {2021-01-06},
  abstract = {Compressed Sensing MRI (CS-MRI) has provided theoretical foundations upon which the time-consuming MRI acquisition process can be accelerated. However, it primarily relies on iterative numerical solvers which still hinders their adaptation in time-critical applications. In addition, recent advances in deep neural networks have shown their potential in computer vision and image processing, but their adaptation to MRI reconstruction is still in an early stage. In this paper, we propose a novel deep learning-based generative adversarial model, RefineGAN, for fast and accurate CS-MRI reconstruction. The proposed model is a variant of fully-residual convolutional autoencoder and generative adversarial networks (GANs), specifically designed for CS-MRI formulation; it employs deeper generator and discriminator networks with cyclic data consistency loss for faithful interpolation in the given under-sampled k-space data. In addition, our solution leverages a chained network to further enhance the reconstruction quality. RefineGAN is fast and accurate -- the reconstruction process is extremely rapid, as low as tens of milliseconds for reconstruction of a 256x256 image, because it is one-way deployment on a feed-forward network, and the image quality is superior even for extremely low sampling rate (as low as 10\%) due to the data-driven nature of the method. We demonstrate that RefineGAN outperforms the state-of-the-art CS-MRI methods by a large margin in terms of both running time and image quality via evaluation using several open-source MRI databases.},
  archivePrefix = {arXiv},
  eprint = {1709.00753},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/SKVMR8TZ/Quan et al. - 2018 - Compressed Sensing MRI Reconstruction using a Gene.pdf;/home/krawczuk/Zotero/storage/4YDZ3JXQ/1709.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,GAN},
  number = {6}
}

@online{ramasingheHowTrainYour2020,
  title = {How to Train Your Conditional {{GAN}}: {{An}} Approach Using Geometrically Structured Latent Manifolds},
  shorttitle = {How to Train Your Conditional {{GAN}}},
  author = {Ramasinghe, Sameera and Farazi, Moshiur and Khan, Salman and Barnes, Nick and Gould, Stephen},
  date = {2020-11-30},
  url = {http://arxiv.org/abs/2011.13055},
  urldate = {2021-01-06},
  abstract = {Conditional generative modeling typically requires capturing one-to-many mappings between the inputs and outputs. However, vanilla conditional GANs (cGAN) tend to ignore the variations of the latent seeds which results in mode-collapse. As a solution, recent works have moved towards comparatively expensive models for generating diverse outputs in a conditional setting. In this paper, we argue that the limited diversity of the vanilla cGANs is not due to a lack of capacity, but a result of non-optimal training schemes. We tackle this problem from a geometrical perspective and propose a novel training mechanism that increases both the diversity and the visual quality of the vanilla cGAN. The proposed solution does not demand architectural modifications and paves the way for more efficient architectures that target conditional generation in multi-modal spaces. We validate the efficacy of our model against a diverse set of tasks and show that the proposed solution is generic and effective across multiple datasets.},
  archivePrefix = {arXiv},
  eprint = {2011.13055},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/K4Y9IE68/Ramasinghe et al. - 2020 - How to train your conditional GAN An approach usi.pdf;/home/krawczuk/Zotero/storage/EDGMXHEF/2011.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,GAN},
  primaryClass = {cs}
}

@article{sunBetterGlobalLoss,
  title = {Towards a {{Better Global Loss Landscape}} of {{GANs}}},
  author = {Sun, Ruoyu and Fang, Tiantian and Schwing, Alex},
  pages = {14},
  abstract = {Understanding of GAN training is still very limited. One major challenge is its non-convex-non-concave min-max objective, which may lead to sub-optimal local minima. In this work, we perform a global landscape analysis of the empirical loss of GANs. We prove that a class of separable-GAN, including the original JS-GAN, has exponentially many bad basins which are perceived as mode-collapse. We also study the relativistic pairing GAN (RpGAN) loss which couples the generated samples and the true samples. We prove that RpGAN has no bad basins. Experiments on synthetic data show that the predicted bad basin can indeed appear in training. We also perform experiments to support our theory that RpGAN has a better landscape than separable-GAN. For instance, we empirically show that RpGAN performs better than separable-GAN with relatively narrow neural nets. The code is available at https://github.com/AilsaF/RS-GAN.},
  file = {/home/krawczuk/Zotero/storage/C5FAUK7T/NeurIPS-2020-towards-a-better-global-loss-landscape-of-gans-Supplemental.pdf;/home/krawczuk/Zotero/storage/SEMSTGCC/Sun et al. - Towards a Better Global Loss Landscape of GANs.pdf},
  keywords = {GAN,mode collapse},
  langid = {english},
  year={2020},
}

@inproceedings{thanh-tungCatastrophicForgettingMode2020a,
  title = {Catastrophic Forgetting and Mode Collapse in {{GANs}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Thanh-Tung, H. and Tran, T.},
  date = {2020-07},
  pages = {1--10},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9207181},
  abstract = {In this paper, we show that Generative Adversarial Networks (GANs) suffer from catastrophic forgetting even when they are trained to approximate a single target distribution. We show that GAN training is a continual learning problem in which the sequence of changing model distributions is the sequence of tasks to the discriminator. The level of mismatch between tasks in the sequence determines the level of forgetting. Catastrophic forgetting is interrelated to mode collapse and can make the training of GANs non-convergent. We investigate the landscape of the discriminator's output in different variants of GANs and find that when a GAN converges to a good equilibrium, real training datapoints are wide local maxima of the discriminator. We empirically show the relationship between the sharpness of local maxima and mode collapse and generalization in GANs. We show how catastrophic forgetting prevents the discriminator from making real datapoints local maxima, and thus causes non-convergence. Finally, we study methods for preventing catastrophic forgetting in GANs.},
  eventtitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  file = {/home/krawczuk/Zotero/storage/56FFHWFU/Thanh-Tung and Tran - 2020 - Catastrophic forgetting and mode collapse in GANs.pdf;/home/krawczuk/Zotero/storage/37TT2CRA/9207181.html;/home/krawczuk/Zotero/storage/KQ3AC2BS/9207181.html},
  keywords = {catastrophic forgetting,continual learning problem,Convergence,Gallium nitride,GAN,GAN training,GANs,GANs nonconvergent,generative,generative adversarial networks,Generative adversarial networks,Generators,learning (artificial intelligence),mode collapse,neural nets,Neural networks,real datapoints local maxima,single target distribution,Task analysis,Training}
}

@inproceedings{yaoSupportMatchingNovel2019,
  title = {Support {{Matching}}: {{A Novel Regularization}} to {{Escape}} from {{Mode Collapse}} in {{GANs}}},
  shorttitle = {Support {{Matching}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Yao, Yinghua and Pan, Yuangang and Tsang, Ivor W. and Yao, Xin},
  editor = {Gedeon, Tom and Wong, Kok Wai and Lee, Minho},
  date = {2019},
  pages = {40--48},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-36808-1_5},
  abstract = {Generative adversarial network (GAN) is an implicit generative model known for its ability to generate sharp images. However, it is poor at generating diverse data, which refers to the mode collapse problem. It turns out that GAN is prone to emphasizing the quality of samples but ignoring their diversity. When mode collapse happens, the support of the generated data distribution is not aligned with that of the real data distribution. We thus propose Support Regularized-GAN (SR-GAN) to address such a mode collapse issue by matching their support. Our experiments on synthetic and real-world datasets show that our regularization can mitigate the mode collapse and also improve the data quality.},
  file = {/home/krawczuk/Zotero/storage/SJVKBVK7/Yao et al. - 2019 - Support Matching A Novel Regularization to Escape.pdf},
  isbn = {978-3-030-36808-1},
  keywords = {GAN,GANs,mode collapse,Mode collapse,Support matching},
  langid = {english},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@online{yuInclusiveGANImproving2020,
  title = {Inclusive {{GAN}}: {{Improving Data}} and {{Minority Coverage}} in {{Generative Models}}},
  shorttitle = {Inclusive {{GAN}}},
  author = {Yu, Ning and Li, Ke and Zhou, Peng and Malik, Jitendra and Davis, Larry and Fritz, Mario},
  date = {2020-08-22},
  url = {http://arxiv.org/abs/2004.03355},
  urldate = {2021-01-06},
  abstract = {Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at GitHub.},
  archivePrefix = {arXiv},
  eprint = {2004.03355},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/AG9DRDWT/Yu et al. - 2020 - Inclusive GAN Improving Data and Minority Coverag.pdf;/home/krawczuk/Zotero/storage/MXJ8JJNA/2004.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,diversity,GAN,minority,mode collapse},
  primaryClass = {cs}
}

@inproceedings{zhangConvergenceModeCollapse2018,
  title = {On the Convergence and Mode Collapse of {{GAN}}},
  booktitle = {{{SIGGRAPH Asia}} 2018 {{Technical Briefs}}},
  author = {Zhang, Zhaoyu and Li, Mengyan and Yu, Jun},
  date = {2018-12-04},
  pages = {1--4},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3283254.3283282},
  url = {https://doi.org/10.1145/3283254.3283282},
  urldate = {2021-01-06},
  abstract = {Generative adversarial network (GAN) is a powerful generative model. However, it suffers from several problems, such as convergence instability and mode collapse. To overcome these drawbacks, this paper presents a novel architecture of GAN, which consists of one generator and two different discriminators. With the fact that GAN is the analogy of a minimax game, the proposed architecture is as follows. The generator (G) aims to produce realistic-looking samples to fool both of two discriminators. The first discriminator (D1) rewards high scores for samples from the data distribution, while the second one (D2) favors samples from the generator conversely. Specifically, the ResBlock and minibatch discrimination (MD) architectures are adopted in D1 to improve the diversity of the samples. The leaky rectified linear unit (Leaky ReLU) and batch normalization (BN) are replaced by the scaled exponential linear unit (SELU) in D2 to alleviate the convergence problem. A new loss function that minimizes the KL divergence is designed to better optimize the model. Extensive experiments on CIFAR-10/100 datasets demonstrate that the proposed method can effectively solve the problems of convergence and mode collapse.},
  file = {/home/krawczuk/Zotero/storage/3FMTNT4Z/Zhang et al. - 2018 - On the convergence and mode collapse of GAN.pdf},
  isbn = {978-1-4503-6062-3},
  keywords = {convergence,GAN,mode collapse},
  series = {{{SA}} '18}
}

@inproceedings{zhangGradientVanishingDivergence2019,
  title = {Towards the {{Gradient Vanishing}}, {{Divergence Mismatching}} and {{Mode Collapse}} of {{Generative Adversarial Nets}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Zhang, Zhaoyu and Luo, Changwei and Yu, Jun},
  date = {2019-11-03},
  pages = {2377--2380},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3357384.3358081},
  url = {https://doi.org/10.1145/3357384.3358081},
  urldate = {2021-01-06},
  abstract = {Generative adversarial network (GAN) is a powerful generative model. However, it suffers from gradient vanishing, divergence mismatching and mode collapse. To overcome these problems, we propose a novel GAN, which consists of one generator G and two discriminators (D1, D2). Focusing on the gradient vanishing, Spectral Normalization (SN) and ResBlock are first adopted in D1 and D2. Then, Scaled Exponential Linear Units (SELU) is adopted at last half layers of D2 to further address the problem. To divergence mismatching, relativistic discriminator is adopted in our GAN to make the loss function minimization in the training of generator equal to the theoretical divergence minimization. Concentrating on the mode collapse, D1 rewards high scores for the samples from the data distribution, while D2 favors the samples from the generator conversely. In addition, the minibatch discrimination is adopted in D1 to further address the problem. Extensive experiments on CIFAR-10/100 and ImageNet datasets demonstrate that our GAN can obtain the highest inception score (IS) and lowest Frechet Inception Distance (FID) compared with other state-of-the-art GANs.},
  file = {/home/krawczuk/Zotero/storage/HWP65PLM/Zhang et al. - 2019 - Towards the Gradient Vanishing, Divergence Mismatc.pdf},
  isbn = {978-1-4503-6976-3},
  keywords = {divergence mismatching,gan,GAN,gradient vanishing,mode collapse},
  series = {{{CIKM}} '19}
}

@article{zhongRethinkingGenerativeModea,
  title = {Rethinking {{Generative Mode Coverage}}: {{A Pointwise Guaranteed Approach}}},
  author = {Zhong, Peilin and Mo, Yuchen and Xiao, Chang and Chen, Pengyu and Zheng, Changxi},
  pages = {12},
  abstract = {Many generative models have to combat missing modes. The conventional wisdom to this end is by reducing through training a statistical distance (such as f -divergence) between the generated distribution and provided data distribution. But this is more of a heuristic than a guarantee. The statistical distance measures a global, but not local, similarity between two distributions. Even if it is small, it does not imply a plausible mode coverage. Rethinking this problem from a game-theoretic perspective, we show that a complete mode coverage is firmly attainable. If a generative model can approximate a data distribution moderately well under a global statistical distance measure, then we will be able to find a mixture of generators that collectively covers every data point and thus every mode, with a lower-bounded generation probability. Constructing the generator mixture has a connection to the multiplicative weights update rule, upon which we propose our algorithm. We prove that our algorithm guarantees complete mode coverage. And our experiments on real and synthetic datasets confirm better mode coverage over recent approaches, ones that also use generator mixtures but rely on global statistical distances.},
  file = {/home/krawczuk/Zotero/storage/Y8I8TQ5E/Zhong et al. - Rethinking Generative Mode Coverage A Pointwise G.pdf},
  keywords = {GAN},
  langid = {english},
  year={2019},
}

@article{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{karras2017progressive,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}



@online{zhouLipschitzGenerativeAdversarial2019,
  ids = {zhouLipschitzGenerativeAdversarial,zhouLipschitzGenerativeAdversarial2019a},
  title = {Lipschitz {{Generative Adversarial Nets}}},
  author = {Zhou, Zhiming and Liang, Jiadong and Song, Yuxuan and Yu, Lantao and Wang, Hongwei and Zhang, Weinan and Yu, Yong and Zhang, Zhihua},
  date = {2019-06-24},
  url = {http://arxiv.org/abs/1902.05687},
  urldate = {2020-11-14},
  abstract = {In this paper, we study the convergence of generative adversarial networks (GANs) from the perspective of the informativeness of the gradient of the optimal discriminative function. We show that GANs without restriction on the discriminative function space commonly suffer from the problem that the gradient produced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lipschitz, does not suffer from such a gradient uninformativeness problem. We further show in the paper that the model with a compact dual form of Wasserstein distance, where the Lipschitz condition is relaxed, may also theoretically suffer from this issue. This implies the importance of Lipschitz condition and motivates us to study the general formulation of GANs with Lipschitz constraint, which leads to a new family of GANs that we call Lipschitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the optimal discriminative function as well as the existence of a unique Nash equilibrium. We prove that LGANs are generally capable of eliminating the gradient uninformativeness problem. According to our empirical analysis, LGANs are more stable and generate consistently higher quality samples compared with WGAN.},
  annotation = {26 citations (Semantic Scholar/arXiv) [2020-12-13]},
  archivePrefix = {arXiv},
  eprint = {1902.05687},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/9XIBRS59/Zhou et al_2019_Lipschitz Generative Adversarial Nets.pdf;/home/krawczuk/Zotero/storage/RM5E2J8N/Zhou et al. - Lipschitz Generative Adversarial Nets.pdf;/home/krawczuk/Zotero/storage/RRK6F7VC/Zhou et al_2019_Lipschitz Generative Adversarial Nets.pdf;/home/krawczuk/Zotero/storage/F7IKWHSZ/1902.html;/home/krawczuk/Zotero/storage/PUJK42LU/1902.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,GAN,quickfind,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}



@online{diengPrescribedGenerativeAdversarial2019,
  title = {Prescribed {{Generative Adversarial Networks}}},
  author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M. and Titsias, Michalis K.},
  date = {2019-10-09},
  url = {http://arxiv.org/abs/1910.04302},
  urldate = {2020-11-06},
  abstract = {Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational autoencoders (VAEs).},
  annotation = {21 citations (Semantic Scholar/arXiv) [2020-12-13]},
  archivePrefix = {arXiv},
  eprint = {1910.04302},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/26KI7WST/Dieng et al. - 2019 - Prescribed Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/49CT3TXT/Dieng et al_2019_Prescribed Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/FX4I6C5G/Dieng et al. - Prescribed Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/NLPU9RK9/Dieng et al_2019_Prescribed Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/HM8EZRN5/1910.html;/home/krawczuk/Zotero/storage/MM4BUBQB/1910.html;/home/krawczuk/Zotero/storage/WKZXKTQG/1910.html},
  keywords = {Computer Science - Machine Learning,GAN,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}


@online{aroraGeneralizationEquilibriumGenerative2017,
  title = {Generalization and {{Equilibrium}} in {{Generative Adversarial Nets}} ({{GANs}})},
  author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
  date = {2017-08-01},
  url = {http://arxiv.org/abs/1703.00573},
  urldate = {2021-01-06},
  abstract = {We show that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a special class of generators with natural training objectives when generator capacity and training set sizes are moderate. This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.},
  archivePrefix = {arXiv},
  eprint = {1703.00573},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/DH9VD6S8/Arora et al. - 2017 - Generalization and Equilibrium in Generative Adver.pdf;/home/krawczuk/Zotero/storage/GTAIUXWM/1703.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}



@article{farniaGANsAlwaysHave,
  title = {Do {{GANs}} Always Have {{Nash}} Equilibria?},
  author = {Farnia, Farzan and Ozdaglar, Asuman},
  pages = {11},
  abstract = {Generative adversarial networks (GANs) represent a zero-sum game between two machine players, a generator and a discriminator, designed to learn the distribution of data. While GANs have achieved state-of-the-art performance in several benchmark learning tasks, GAN minimax optimization still poses great theoretical and empirical challenges. GANs trained using first-order optimization methods commonly fail to converge to a stable solution where the players cannot improve their objective, i.e., the Nash equilibrium of the underlying game. Such issues raise the question of the existence of Nash equilibria in GAN zero-sum games. In this work, we show through theoretical and numerical results that indeed GAN zero-sum games may have no Nash equilibria. To characterize an equilibrium notion applicable to GANs, we consider the equilibrium of a new zerosum game with an objective function given by a proximal operator applied to the original objective, a solution we call the proximal equilibrium. Unlike the Nash equilibrium, the proximal equilibrium captures the sequential nature of GANs, in which the generator moves first followed by the discriminator. We prove that the optimal generative model in Wasserstein GAN problems provides a proximal equilibrium. Inspired by these results, we propose a new approach, which we call proximal training, for solving GAN problems. We perform several numerical experiments indicating the existence of proximal equilibria in GANs.},
  file = {/home/krawczuk/Zotero/storage/4EPS69XY/Farnia and Ozdaglar - Do GANs always have Nash equilibria.pdf},
  keywords = {GAN},
  langid = {english},
  year={2020}
}



@article{liuSpectralRegularizationCombating,
  title = {Spectral {{Regularization}} for {{Combating Mode Collapse}} in {{GANs}}},
  author = {Liu, Kanglin and Tang, Wenming and Zhou, Fei and Qiu, Guoping},
  pages = {9},
  abstract = {Despite excellent progress in recent years, mode collapse remains a major unsolved problem in generative adversarial networks (GANs). In this paper, we present spectral regularization for GANs (SR-GANs), a new and robust method for combating the mode collapse problem in GANs. Theoretical analysis shows that the optimal solution to the discriminator has a strong relationship to the spectral distributions of the weight matrix. Therefore, we monitor the spectral distribution in the discriminator of spectral normalized GANs (SN-GANs), and discover a phenomenon which we refer to as spectral collapse, where a large number of singular values of the weight matrices drop dramatically when mode collapse occurs. We show that there are strong evidence linking mode collapse to spectral collapse; and based on this link, we set out to tackle spectral collapse as a surrogate of mode collapse. We have developed a spectral regularization method where we compensate the spectral distributions of the weight matrices to prevent them from collapsing, which in turn successfully prevents mode collapse in GANs. We provide theoretical explanations for why SRGANs are more stable and can provide better performances than SN-GANs. We also present extensive experimental results and analysis to show that SR-GANs not only always outperform SN-GANs but also always succeed in combating mode collapse where SN-GANs fail.},
  file = {/home/krawczuk/Zotero/storage/XEDKRDIT/Liu et al. - Spectral Regularization for Combating Mode Collaps.pdf},
  langid = {english},
  year={2019},
}



@online{khorramshahiGANsVariationalEntropy2020,
  title = {{{GANs}} with {{Variational Entropy Regularizers}}: {{Applications}} in {{Mitigating}} the {{Mode}}-{{Collapse Issue}}},
  shorttitle = {{{GANs}} with {{Variational Entropy Regularizers}}},
  author = {Khorramshahi, Pirazh and Souri, Hossein and Chellappa, Rama and Feizi, Soheil},
  date = {2020-09-24},
  url = {http://arxiv.org/abs/2009.11921},
  urldate = {2021-01-04},
  abstract = {Building on the success of deep learning, Generative Adversarial Networks (GANs) provide a modern approach to learn a probability distribution from observed samples. GANs are often formulated as a zero-sum game between two sets of functions; the generator and the discriminator. Although GANs have shown great potentials in learning complex distributions such as images, they often suffer from the mode collapse issue where the generator fails to capture all existing modes of the input distribution. As a consequence, the diversity of generated samples is lower than that of the observed ones. To tackle this issue, we take an information-theoretic approach and maximize a variational lower bound on the entropy of the generated samples to increase their diversity. We call this approach GANs with Variational Entropy Regularizers (GAN+VER). Existing remedies for the mode collapse issue in GANs can be easily coupled with our proposed variational entropy regularization. Through extensive experimentation on standard benchmark datasets, we show all the existing evaluation metrics highlighting difference of real and generated samples are significantly improved with GAN+VER.},
  archivePrefix = {arXiv},
  eprint = {2009.11921},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/EZXZZ4CD/Khorramshahi et al. - 2020 - GANs with Variational Entropy Regularizers Applic.pdf;/home/krawczuk/Zotero/storage/3QMVBH4A/2009.html},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,GAN,mode collapse,Statistics - Machine Learning},
  primaryClass = {cs, eess, math, stat}
}



@online{chavdarovaTamingGANsLookaheadMinmax2020,
  ids = {chavdarovaTamingGANsLookaheadMinmax2020a,chavdarovaTamingGANsLookaheadMinmax2020b},
  title = {Taming {{GANs}} with {{Lookahead}}-{{Minmax}}},
  author = {Chavdarova, Tatjana and Pagliardini, Matteo and Stich, Sebastian U. and Fleuret, Francois and Jaggi, Martin},
  date = {2020-10-26},
  url = {http://arxiv.org/abs/2006.14567},
  urldate = {2021-01-06},
  abstract = {Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead–minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead–minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.},
  archivePrefix = {arXiv},
  eprint = {2006.14567},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/5LC87A8Y/Chavdarova et al. - 2020 - Taming GANs with Lookahead-Minmax.pdf;/home/krawczuk/Zotero/storage/PL2AAQ4G/Chavdarova et al. - 2020 - Taming GANs with Lookahead-Minmax.pdf;/home/krawczuk/Zotero/storage/UDBV7TUV/Chavdarova et al. - 2020 - Taming GANs with Lookahead-Minmax.pdf},
  keywords = {Computer Science - Machine Learning,GAN,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}



@article{srivastavaVEEGANReducingMode2017,
  ids = {srivastavaVEEGANReducingMode,srivastavaVEEGANReducingModea},
  title = {{{VEEGAN}}: {{Reducing Mode Collapse}} in {{GANs}} Using {{Implicit Variational Learning}}},
  shorttitle = {{{VEEGAN}}},
  author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
  date = {2017},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {30},
  pages = {3308--3318},
  url = {https://papers.nips.cc/paper/2017/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html},
  urldate = {2021-01-03},
  file = {/home/krawczuk/Zotero/storage/5N5PIXCI/Srivastava et al. - VEEGAN Reducing Mode Collapse in GANs using Impli.pdf;/home/krawczuk/Zotero/storage/E95ZXWE2/Srivastava et al. - VEEGAN Reducing Mode Collapse in GANs using Impli.pdf;/home/krawczuk/Zotero/storage/G9G9F4XI/Srivastava et al. - 2017 - VEEGAN Reducing Mode Collapse in GANs using Impli.pdf},
  langid = {english}
}
@ARTICLE{muckleyStateoftheArtMachineLearning2020,
  author={Muckley, Matthew J. and Riemenschneider, Bruno and Radmanesh, Alireza and Kim, Sunwoo and Jeong, Geunu and Ko, Jingyu and Jun, Yohan and Shin, Hyungseob and Hwang, Dosik and Mostapha, Mahmoud and Arberet, Simon and Nickel, Dominik and Ramzi, Zaccharie and Ciuciu, Philippe and Starck, Jean-Luc and Teuwen, Jonas and Karkalousos, Dimitrios and Zhang, Chaoping and Sriram, Anuroop and Huang, Zhengnan and Yakubova, Nafissa and Lui, Yvonne W. and Knoll, Florian},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Results of the 2020 fastMRI Challenge for Machine Learning MR Image Reconstruction}, 
  year={2021},
  volume={40},
  number={9},
  pages={2306-2317},
  doi={10.1109/TMI.2021.3075856}}

@online{pezzottiAdaptiveCSNetFastMRIAdaptive2019,
  title = {Adaptive-{{CS}}-{{Net}}: {{FastMRI}} with {{Adaptive Intelligence}}},
  shorttitle = {Adaptive-{{CS}}-{{Net}}},
  author = {Pezzotti, Nicola and de Weerdt, Elwin and Yousefi, Sahar and Elmahdy, Mohamed S. and van Gemert, Jeroen and Schülke, Christophe and Doneva, Mariya and Nielsen, Tim and Kastryulin, Sergey and Lelieveldt, Boudewijn P. F. and van Osch, Matthias J. P. and Staring, Marius},
  date = {2019-12-13},
  url = {http://arxiv.org/abs/1912.12259},
  urldate = {2021-01-06},
  abstract = {Adaptive intelligence aims at empowering machine learning techniques with the extensive use of domain knowledge. In this work, we present the application of adaptive intelligence to accelerate MR acquisition. Starting from undersampled k-space data, an iterative learning-based reconstruction scheme inspired by compressed sensing theory is used to reconstruct the images. We adopt deep neural networks to refine and correct prior reconstruction assumptions given the training data. Our results show that an adaptive intelligence approach performs better than traditional methods as well as deep learning methods that do not take prior knowledge into account.},
  archivePrefix = {arXiv},
  eprint = {1912.12259},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/CJ4CILAG/Pezzotti et al. - 2019 - Adaptive-CS-Net FastMRI with Adaptive Intelligenc.pdf;/home/krawczuk/Zotero/storage/B95RUZE3/1912.html},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  options = {useprefix=true},
  primaryClass = {eess}
}

@misc{zbontarFastMRIOpenDataset2019,
  title = {{{fastMRI}}: {{An Open Dataset}} and {{Benchmarks}} for {{Accelerated MRI}}},
  shorttitle = {{{fastMRI}}},
  author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
  date = {2019-12-11},
  year={2019},
  url = {http://arxiv.org/abs/1811.08839},
  urldate = {2021-01-06},
  abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
  archivePrefix = {arXiv},
  eprint = {1811.08839},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/AIQZAY2X/Zbontar et al. - 2019 - fastMRI An Open Dataset and Benchmarks for Accele.pdf;/home/krawczuk/Zotero/storage/U9KRI2I5/1811.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Physics - Medical Physics,Statistics - Machine Learning},
  primaryClass = {physics, stat}
}


@article{putzkyInvertLearnInverta,
  title={Invert to learn to invert},
  author={Putzky, Patrick and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={446--456},
  year={2019}
}


@ARTICLE{yang_dagan_2018,  
  author={Yang, Guang and Yu, Simiao and Dong, Hao and Slabaugh, Greg and Dragotti, Pier Luigi and Ye, Xujiong and Liu, Fangde and Arridge, Simon and Keegan, Jennifer and Guo, Yike and Firmin, David}, 
  journal={IEEE Transactions on Medical Imaging}, 
  title={DAGAN: Deep De-Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction}, 
  year={2018},
  volume={37},
  number={6},
  pages={1310-1321}, 
  doi={10.1109/TMI.2017.2785879}}

@article{yuanSARAGANSelfAttentionRelative2020,
  title = {{{SARA}}-{{GAN}}: {{Self}}-{{Attention}} and {{Relative Average Discriminator Based Generative Adversarial Networks}} for {{Fast Compressed Sensing MRI Reconstruction}}},
  shorttitle = {{{SARA}}-{{GAN}}},
  author = {Yuan, Zhenmou and Jiang, Mingfeng and Wang, Yaming and Wei, Bo and Li, Yongming and Wang, Pin and Menpes-Smith, Wade and Niu, Zhangming and Yang, Guang},
  date = {2020-11-26},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front Neuroinform},
  volume = {14},
  issn = {1662-5196},
  doi = {10.3389/fninf.2020.611666},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7726262/},
  urldate = {2021-01-08},
  abstract = {Research on undersampled magnetic resonance image (MRI) reconstruction can increase the speed of MRI imaging and reduce patient suffering. In this paper, an undersampled MRI reconstruction method based on Generative Adversarial Networks with the Self-Attention mechanism and the Relative Average discriminator (SARA-GAN) is proposed. In our SARA-GAN, the relative average discriminator theory is applied to make full use of the prior knowledge, in which half of the input data of the discriminator is true and half is fake. At the same time, a self-attention mechanism is incorporated into the high-layer of the generator to build long-range dependence of the image, which can overcome the problem of limited convolution kernel size. Besides, spectral normalization is employed to stabilize the training process. Compared with three widely used GAN-based MRI reconstruction methods, i.e., DAGAN, DAWGAN, and DAWGAN-GP, the proposed method can obtain a higher peak signal-to-noise ratio (PSNR) and structural similarity index measure(SSIM), and the details of the reconstructed image are more abundant and more realistic for further clinical scrutinization and diagnostic tasks.},
  eprint = {33324189},
  eprinttype = {pmid},
  file = {/home/krawczuk/Zotero/storage/YNCDI5FV/Yuan et al. - 2020 - SARA-GAN Self-Attention and Relative Average Disc.pdf},
  keywords = {GAN},
  pmcid = {PMC7726262}
}

@inproceedings{zhangSelfAttentionGenerativeAdversarial2019b,
  title = {Self-{{Attention Generative Adversarial Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  date = {2019-05-24},
  pages = {7354--7363},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v97/zhang19d.html},
  urldate = {2021-01-08},
  abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolution...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {/home/krawczuk/Zotero/storage/6ISLYF4N/Zhang et al. - 2019 - Self-Attention Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/LDICHA9E/zhang19d.html},
  keywords = {GAN},
  langid = {english}
}



@online{gohUnderstandingIntegratedGradients2020a,
  title = {Understanding {{Integrated Gradients}} with {{SmoothTaylor}} for {{Deep Neural Network Attribution}}},
  author = {Goh, Gary S. W. and Lapuschkin, Sebastian and Weber, Leander and Samek, Wojciech and Binder, Alexander},
  date = {2020-04-22},
  url = {http://arxiv.org/abs/2004.10484},
  urldate = {2021-01-04},
  abstract = {Integrated gradients as an attribution method for deep neural network models offers simple implementability. However, it also suffers from noisiness of explanations, which affects the ease of interpretability. In this paper, we present Smooth Integrated Gradients as a statistically improved attribution method inspired by Taylor's theorem, which does not require a fixed baseline to be chosen. We apply both methods to the image classification problem, using the ILSVRC2012 ImageNet object recognition dataset, and a couple of pretrained image models to generate attribution maps of their predictions. These attribution maps are visualized by saliency maps which can be evaluated qualitatively. We also empirically evaluate them using quantitative metrics such as perturbations-based score drops and multi-scaled total variance. We further propose adaptive noising to optimize for the noise scale hyperparameter value in our proposed method. From our experiments, we find that the Smooth Integrated Gradients approach together with adaptive noising is able to generate better quality saliency maps with lesser noise and higher sensitivity to the relevant points in the input space.},
  archivePrefix = {arXiv},
  eprint = {2004.10484},
  eprinttype = {arxiv},
  file = {/home/krawczuk/Zotero/storage/SPND6QB6/Goh et al. - 2020 - Understanding Integrated Gradients with SmoothTayl.pdf;/home/krawczuk/Zotero/storage/XIBWAEC6/2004.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}


@article{gautam2020masking,
  title={Masking schemes for universal marginalisers},
  author={Gautam, Divya and Lomeli, Maria and Gourgoulias, Kostis and Thompson, Daniel H and Johri, Saurabh},
  journal={arXiv preprint arXiv:2001.05895},
  year={2020}
}

@article{douglas2017universal,
  title={A universal marginalizer for amortized inference in generative models},
  author={Douglas, Laura and Zarov, Iliyan and Gourgoulias, Konstantinos and Lucas, Chris and Hart, Chris and Baker, Adam and Sahani, Maneesh and Perov, Yura and Johri, Saurabh},
  journal={arXiv preprint arXiv:1711.00695},
  year={2017}
}


@article{strauss2021arbitrary,
  title={Arbitrary Conditional Distributions with Energy},
  author={Strauss, Ryan R and Oliva, Junier B},
  journal={arXiv preprint arXiv:2102.04426},
  year={2021}
}


@inproceedings{hussein2020image,
  title={Image-adaptive gan based reconstruction},
  author={Hussein, Shady Abu and Tirer, Tom and Giryes, Raja},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={3121--3129},
  year={2020}
}

@inproceedings{jalali2019solving,
  title={Solving linear inverse problems using generative models},
  author={Jalali, Shirin and Yuan, Xin},
  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)},
  pages={512--516},
  year={2019},
  organization={IEEE}
}

@article{anirudh2018unsupervised,
  title={An unsupervised approach to solving inverse problems using generative adversarial networks},
  author={Anirudh, Rushil and Thiagarajan, Jayaraman J and Kailkhura, Bhavya and Bremer, Timo},
  journal={arXiv preprint arXiv:1805.07281},
  year={2018}
}

@article{ardizzone2018analyzing,
  title={Analyzing inverse problems with invertible neural networks},
  author={Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W and Klessen, Ralf S and Maier-Hein, Lena and Rother, Carsten and K{\"o}the, Ullrich},
  journal={arXiv preprint arXiv:1808.04730},
  year={2018}
}

@article{jarrett2020inverse,
  title={Inverse active sensing: Modeling and understanding timely decision-making},
  author={Jarrett, Daniel and Van Der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2006.14141},
  year={2020}
}


@inproceedings{asim2020invertible,
  title={Invertible generative models for inverse problems: mitigating representation error and dataset bias},
  author={Asim, Muhammad and Daniels, Max and Leong, Oscar and Ahmed, Ali and Hand, Paul},
  booktitle={International Conference on Machine Learning},
  pages={399--409},
  year={2020},
  organization={PMLR}
}

@inproceedings{sim2020optimal,
  title={Optimal Transport Structure of CycleGAN for Unsupervised Learning for Inverse Problems},
  author={Sim, Byeongsu and Oh, Gyutaek and Ye, Jong Chul},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8644--8647},
  year={2020},
  organization={IEEE}
}
