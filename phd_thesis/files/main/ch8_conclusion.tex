%!TEX root = ../../thesis.tex
\cleardoublepage
\chapter{Conclusion and future works}\label{ch:conclusion}
\markboth{Conclusion and future works}{Conclusion and future works}
%\addcontentsline{toc}{chapter}{Conclusion and future works}

\section*{Summary of the thesis}

In this thesis, we studied the problem of optimizing sampling patterns for Cartesian MRI. More specifically, we considered the problem of optimizing sampling patterns in a sequential context, using learning-based, data-driven methods. Our work consists of both methodological novelties and insights into what impacts the performance of models, what design choices matter most and \textit{why} some methods generally perform worse than others. 

In \textbf{Chapter \ref{ch:sampling}}, we brought a taxonomy of approaches that have been taken to tackle the problem of optimizing sampling patterns, which helped shed light on the development of approaches to optimize sampling for Cartesian MRI. More concretely, we proposed that similarly to reconstruction methods, the field of mask optimization has undergone a shift from \textit{model-based} to \textit{learning-based} approaches \citep{ravishankar2019image,doneva2020mathematical}, but that it has also shifted from \textit{model-driven} towards \textit{data-driven} criteria to evaluate the quality of sampling masks. It was naturally observed that moving toward learning-based, data-driven approaches enables the model to perform better \citep{chauffert2013variable,zijlstra2016evaluation}, a trend that was also consistently confirmed throughout our work. We also observed that within the realm of learning-based, data-driven methods, several trends arose, where researchers tackled the problem of learning to design masks as either a fixed, static problem or a sequential, dynamic approach. Following the sequential approach to mask design, our work focused on studying the problem of optimizing sampling on pre-trained reconstruction models. This constitutes the precise framework of our contributions. 

Within this context, in \textbf{Chapter \ref{chap:lbcs}}, we first investigated algorithms to drastically improve the scaling of LBCS \citep{gozcu2018learning}.  We showed that ideas from submodular optimization \citep{krause2014submodular} can be leveraged to scale up LBCS by several orders of magnitude, and in some cases without degrading its performance. Our algorithms, sLBCS and lLBCS, have been shown to provide strong mask designs for a range of clinically relevant MRI settings, including multicoil dynamic MRI and 3D MRI. These settings were previously not accessible to LBCS, due to the prohibitive cost of evaluating all candidate locations on the full dataset. sLBCS tackled this limitation by proposing a stochastic sampling of both candidate locations and data points, and achieved results on par with LBCS, and even sometimes outperforming the method. This method is easily implemented and broadly applicable, and we used it as a strong baseline throughout the rest of the thesis. lLBCS was proposed to address the case of multicoil and 3D MRI, where LBCS cannot be evaluated, due to its prohibitive computational cost. By using lazy evaluations, lLBCS can scale up to these problems. lLBCS presents very strong performances at high undersampling rates, but we hypothesized that its list of upper bounds becomes gradually worse as more sampling locations are added to the mask, making its performance gradually worse than baselines. However, several options were proposed to further improve its performance and scalability, and should be investigated as future work.

Our work illustrates the benefit of a flexible learning-based, data-driven approach to the problem of optimizing sampling. Moreover, by focusing on the problem of optimizing sampling patterns on pre-trained reconstruction models, sLBCS and lLBCS integrate a future-proof dimension, as they will remain readily available to any reconstruction method that is applicable to a sequence of sampling rates.

In \textbf{Chapter \ref{ch:rl_mri}}, we aimed at resolving a seeming conflict between mask design methods that used deep reinforcement learning \citep{pineda2020active,bakker2020experimental}. These methods showed a potential to improve over sLBCS and lLBCS by integrating long-horizon sampling as well as patient-adaptive decisions. However, \citet{pineda2020active} and \citet{bakker2020experimental} reached seemingly contradictory conclusions. \citet{pineda2020active} suggested that long-term planning was the most important component, as their Dataset-Specific DDQN (non-adaptive) policy often matched the performance of their Subject-Specific DDQN (adaptive) policy. On the side of \citet{bakker2020experimental}, it was found that a \textit{greedily} trained adaptive deep RL policy could match the performance of policies trained with a long-horizon dependency. In other words, \textit{does the ability to plan multiple steps ahead matter most, as suggested by \citet{pineda2020active}, or is it rather the ability to adapt sampling to a given patient, as suggested by \citet{bakker2020experimental}?} 

Our methodology was simply to consider a baseline policy that is greedily trained (no long-term planning) and non-adaptive, but that is trained to maximize the same reward function as considered by \citet{pineda2020active,bakker2020experimental}. This baseline could then serve as the reference point from which one could observe what brings most performance improvement. However, it became quickly clear that our previously proposed algorithm, sLBCS, could exactly fulfill this role, and it was then natural to use it as the comparison baseline.  Exhaustive experiments allowed us to bring elements of answers to this important question, albeit in a surprising way: it seems indeed that \textit{neither long-term planning, nor adaptive sampling} are able to bring a strong return on investment over sLBCS. 

Exploring this question led us to consider the different processing choices that were done in the works of \citet{pineda2020active} and \citet{bakker2020experimental}, such as the choices of architecture, how the models were pretrained, the metrics used for reporting and how results were aggregated and presented. Surprisingly again, we showed that \textit{inconsistencies in reporting could easily lead to incoherent results}, and that subtle changes in the processing or the reporting pipeline could lead to cases where the SotA deep RL methods of \citet{pineda2020active,bakker2020experimental} were outperformed by sLBCS.


Moreover, the return on investment of moving from sLBCS to a deep RL policy model was at best marginal compared to other changes such as the architecture or the mask distribution used to pretrain it. Indeed, our results show very clearly that the benefit of using a deep RL policy over sLBCS is very limited compared to the improvements achieved by improving the reconstruction architecture or using a distribution of sampling masks tailored to the acquisition horizon considered. However, there is a clear benefit of moving from hand-crafted heuristics such as random sampling and low-pass acquisition policies to a \textit{learning-based}, \textit{data-driven} such as sLBCS. 

These observations led us to wonder whether these results are due to current limitations of deep RL policies or if they describe some more fundamental limitations of the problem. We believe the second option to be more plausible, in light of more recent works such as \citet{bakker2021learning}, where in their case, the best performing adaptive policies \textit{explicitly learn} to become non-adaptive. This also opens some further questions: could we design even simpler \textit{learning-based}, \textit{data-driven} approaches that retain the benefits of sLBCS, while being even easier to train? Can we also find new paradigms of acquisition that would allow to further improve over the \textit{learning-based}, \textit{data-driven} paradigm?



Finally, in \textbf{Chapter \ref{ch:gans}}, we proposed a novel approach to sampling, based on Bayesian modeling. Inspired by previous works to model inverse problems in a Bayesian fashion using GANs \citep{adler2018deep}, we showed that in the context of MRI, this Bayesian modeling allows us to not only perform reconstruction and uncertainty quantification, but also provides a \textit{natural} way to perform adaptive sampling. Indeed, we show that acquiring the location with the largest posterior variance provided a good, natural policy, without the model ever being trained explicitly to perform adaptive sampling. Our GAN-based approach was however outperformed by sLBCS and RL-based policies, but did outperform the approach of \citet{zhang2019reducing}. We postulated that the reason for this is that our GAN approach and the one of \citet{zhang2019reducing} rely on $0$-step information, i.e. base their decision on the current error, rather than receiving feedback about how their decision \textit{actually} performed, which we describe as $1$- or $N$-step information. 

Nonetheless, this last work provides a promising all-in-one approach to Bayesian modeling of inverse problems, and would deserve to be investigated in greater depth. GAN-based approach have generally achieved strong performances in MRI reconstruction \citep{chen2022ai}, and seem like a promising approach to provide an integrated approach to MRI reconstruction, uncertainty quantification and sampling. But as our methodology is tied to learning the posterior rather than GANs themselves, we anticipate that novel contributions to Bayesian modeling would also benefit from these observations, further giving credit to learning the full posterior distribution rather than the pixel-wise mean and variance.

\section*{Insights}
In addition to the methodological contributions, our work provides insights into how sampling policies can be designed efficiently. Figure \ref{fig:summarize_contributions} shows the connections that we established in this thesis. We used the common notions of fixed (non-adaptive) and adaptive policies and refined the distinction between greedy and long-horizon policies to include the \textit{information horizon} accessible to the policy, distinguishing between $0$-step and $1$-step greedy policies. The first are driven by the error at the current reconstruction, whereas the second integrates feedback about the \textit{actual} effect on reconstruction of adding a sampling location to the mask. 

We found that this is the most significant distinction between the existing methods, as in Chapter 6, our results suggest that at least in their current state, neither long-horizon planning ($N$-steps information horizon) nor adaptivity allow significant and consistent improvements over (s)LBCS, a $1$-step non-adaptive policy. The distinction between 0-step and 1-step methods was explored in Chapter 7, where we showed that even an adaptive oracle that acquires the line with the largest current mean-squared error is outperformed by a non-adaptive policy that leverages $1$-step information.

Overall, our experiments suggest that the most important structure that a policy should include in order to achieve near state-of-the-art performance is $1$-step information, i.e. taking into account the \textit{real} impact on performance of acquiring a new location. Further refinements, at least in the current state, seem to bring at most very modest performance improvements, challenging the common trend to design ever-more complex models. 


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{summarize_contributions}
    \caption{Summary of the contributions of this thesis relating to the design of a mask optimization algorithm. }\label{fig:summarize_contributions}
\end{figure}
\newpage
%\todoi{Refine from here onwards. I think that a section on insights would be welcome after the GAN chapter summary. cf. BBNB p. 190 -> What kind of structure matters the most in learning-based, data-driven Cartesian MRI sampling? Our results suggest that it is the 1-step information. Indeed, the 1-step information allows the policy to receive feedback on how a choice actually performed throughout training, which is not the case of 0-step information. About N-step information, we did not observe a very significant performance improvement by moving from 1-step to N-step methods (cf. Chapter \ref{ch:rl_mri}). 
%We could do a drawing/table of the summary with $$\text{0-step adaptive} < \text{1-step non-adaptive} \lesssim \text{N-step adaptive and N-step non-adaptive and 1-step adaptive}$$

%These results seem challenge the common wisdom that larger and more complex models lead to better performance, and illustrate the power of simple methods for MRI sampling. However, it is not possible to entirely rule out complex models without answering further questions. 

%Is the low return of investment of complex deep RL methods applied to MRI intrinsic to them or is it due to their current limitations or weaknesses? Could one explore a different way to train policies that could disprove our current observations? Is there a way to formally prove these empirical observations?}

%\todoi{Put back in context with the questions of the introduction: sLBCS provides a scalable and efficient optimization algorithm and manages to be efficient wrt RL algorithms while retaining most of their performance. We could dwell further on how expensive RL methods are to drive the point further. 
%I think that we could also discuss the GAN contribution in a future-proof vein: the GAN serves as a proof of concept that Bayesian modeling of inverse problems brings a great added value. It allows to do reconstruction and uncertainty quantification and simultaneously provides an adaptive sampling policy. Future generative models could make use of this principle to optimize sampling naturally with Bayesian modeling.}

\section*{Future works}
Nevertheless, even if our thesis brings methodological contributions and insights to the problem of learning sampling masks for Cartesian MRI, many important questions still remain open. 
%Note that our results do not entirely rule out the value of such complex models. Indeed, this work does \textit{not} provide a definitive answer to whether the low return on investment of deep RL methods applied to MRI results from current limitation or if it arises from an intrinsic structure of the problem.


This thesis does \textit{not} provide a definitive answer to whether the low return on investment of deep RL methods applied to MRI results from current limitation or if it arises from an intrinsic structure of the problem. It is not impossible that further advances in deep learning would allow to efficiently train complex models, and to achieve significant performance gain over methods such as (s)LBCS. Similarly, although the results of this thesis provide evidence toward the validity of the conclusions of Figure \ref{fig:summarize_contributions}, it is not impossible either that a deeper connection would be able to explain these observations more adequately. As a result, further work, both at the theoretical and practical levels would be necessary in order to understand what really matters in sampling mask optimization.

Moreover, our results focus on \textit{pre-trained reconstruction models} for sequential acquisitions, and it is not clear how \textit{jointly} training the reconstruction and the policy models might affect the picture given in Figure \ref{fig:summarize_contributions}. Indeed, joint training is challenging as it adds variance to the training procedure, and it is difficult to estimate its impact.
%Mention additional difficulties?
Although a few works considered this problem \citep{jin2019self,van2021active,yin2021end}, it remains to be understood whether joint training mainly results in stronger reconstruction models by enabling them to focus on a more restricted set of masks, in stronger policies by co-designing them with the reconstruction model, or if both, or none, of these conclusions hold. Answering these questions would be an important step towards designing end-to-end, data-driven MRI pipelines. 

%It is an open question whether there is a fundamental, intrinsic limitation to the performance of sampling optimization methods, or whether this performance stems from limitations in the training of these complex models. Further investigations would be necessary to address these questions

%to whether the low return on investment of deep RL methods applied to MRI results from current limitation or if it arises from an intrinsic structure of the problem

A crucial interrogation relates however to the clinical value of optimized sampling masks for MRI sampling. It is known  that there are some significant artifacts introduced by prospective undersampling, but their effect has not been sufficient studied in the case of deep learning-based MRI reconstruction \citep{yu2022validation}. However, there have not been any studies that consider the distortion of a prospective acquisition when using an optimized sampling. Are there ways to design masks that are robust to the distortions of prospective sampling and can retain good performance? Although robustness is a growing concern in MRI reconstruction \citep{antun2020instabilities,johnson2021evaluation,darestani2021measuring}, most works only consider retrospective undersampling.   

Additionally, the world of MR imaging is not restricted to Cartesian imaging, and many exciting recent works also aim at optimizing sampling trajectories for non-Cartesian acquisitions \citep{lazarus2019sparkling,weiss2019pilot,wang2021b,chaithya2022benchmarking}. Non-Cartesian acquisitions can be in practice much faster than Cartesian trajectories, and although they suffer from different limitations, they are definitely a topic that would be worth further investigating.

Beyond these more conceptual directions, many technical questions need also to be addressed for the field of MRI sampling optimization to really mature into a clinically relevant application. We should clarify what really matters at different levels, first in terms of metrics: what kind of metrics do we want to use? Where does the reconstruction quality matters? In terms of the problems considered themselves: should we care about sequential reconstruction or optimize a fixed mask? Do we want to jointly train reconstruction methods and sampling policies? How can we ensure that a trained policy would be robust in a prospective acquisition?

These questions leave many avenues of research open, and promises exciting discoveries in the years to come.

%muckleyStateoftheArtMachineLearning2020 recht2020using

%feng20185d -+ Importance of non-cartesian MRI feng2016xd -> Free breathing
% chaithya2021hybrid -> Non-Cartesian MRI along with PILOT (weiss2019pilot) and BJORK (wang2021b) + chaithya2022benchmarking interesting chaithya2020optimizing -> FUll 3D sparkling
% Technical improvements: investigate joint design of sampling policies and reconstruction models to really develop an end-to-end pipeline. But might be trickier than expected \citep{chaithya2021hybrid} -> For NC it turns out that the learning schedule for joint training matters a great deal, but this has not been studied as much in the Cartesian case. 
% Need also a better investigation. In joint training, it is not immediately clear whether the performance would come from an improved policy or from an improved training of the reconstruction. In other words, does the policy itself bring added value or does it provide a curriculus that enables the reconstruction model to be trained more efficiently ? 
% It would also be necessary to develop criteria that match is relevant to clinicians. This work focused heavily on designing policies that lead to good quality images. However, many downstream tasks such as segmentation and classification could be of interest to the clinicians. Some evidence of task-adapted approaches \citet{huijben2020learning,weiss2019pilot} show that sampling patterns can considerably vary depending on the downstream task at hand. In particular, \citet{weiss2019learning} illustrate how the best policy for downstream segmentation yields images that present large artefacts. In keeping a truly end-to-end mentality, we should make sure that by adopting such reconstruction-oriented sampling policies, we would not end up losing information that machine learning algorithms would require in order to accurately perform segmentation or classification tasks. 
% On non-Cartesian. Non-Cartesian works have the potential to lead to much larger acceleration factors than traditional Cartesian designs \citep{lazarus2019sparkling,chaithya2020optimizing}. In addition, they can be resilient to motion and enable efficient free-breathing body imaging \citep{feng2016xd,feng20185d}.
 
%With the progresses of deep learning-based reconstruction methods, a flurry of works 
